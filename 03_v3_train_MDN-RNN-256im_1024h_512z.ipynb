{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T15:16:12.961239Z",
     "start_time": "2018-05-10T15:16:12.955191Z"
    }
   },
   "source": [
    "This used latent space of 1024 and hidden of only 512. But it seems to really struggle because of the smaller hidden space.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T01:45:11.976359Z",
     "start_time": "2018-05-18T01:45:11.672779Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T01:45:12.355532Z",
     "start_time": "2018-05-18T01:45:11.979074Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "import torch.utils.data\n",
    "\n",
    "# load as dask array\n",
    "import dask.array as da\n",
    "import dask\n",
    "import h5py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T01:45:12.499379Z",
     "start_time": "2018-05-18T01:45:12.357683Z"
    }
   },
   "outputs": [],
   "source": [
    "from vae import VAE5, loss_function_vae\n",
    "from helpers.summarize import TorchSummarizeDf\n",
    "from helpers.dataset import load_cache_data\n",
    "from rnn import MDNRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T12:17:19.585326Z",
     "start_time": "2018-05-09T12:17:19.580159Z"
    }
   },
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T01:45:12.518269Z",
     "start_time": "2018-05-18T01:45:12.501374Z"
    }
   },
   "outputs": [],
   "source": [
    "cuda= torch.cuda.is_available()\n",
    "env_name='sonic256'\n",
    "num_epochs=200\n",
    "batch_size = 3\n",
    "\n",
    "\n",
    "\n",
    "# RNN\n",
    "action_dim = 12\n",
    "seq_len = 6\n",
    "image_size=256\n",
    "chunksize=seq_len*200\n",
    "\n",
    "# VAE loss function\n",
    "gamma = 0.25\n",
    "C = 0\n",
    "\n",
    "# loss function weights\n",
    "lambda_vae = 1/12000\n",
    "lambda_finv = 1/20\n",
    "\n",
    "data_cache_file = '/data/tmp/sonic_rnn_256_v2.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T13:15:12.665594Z",
     "start_time": "2018-05-16T13:15:12.651859Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T01:45:52.349801Z",
     "start_time": "2018-05-18T01:45:52.328363Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7f5d6011d4e0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f5d6011da90>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader_train, loader_test = load_cache_data(\n",
    "    basedir='/data/vae', \n",
    "    env_name='sonic256', \n",
    "    data_cache_file=data_cache_file, \n",
    "    image_size=image_size, \n",
    "    chunksize=chunksize, \n",
    "    action_dim=action_dim,\n",
    "    batch_size=batch_size,\n",
    "    seq_len=seq_len\n",
    ")\n",
    "loader_train, loader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-17T14:32:53.531514Z",
     "start_time": "2018-05-17T14:32:53.515001Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T01:46:04.935086Z",
     "start_time": "2018-05-18T01:45:59.047712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded checkpoint_file ./models/RNN_v3b_256im_512z-joint-training_state_dict.pkl\n",
      "loaded save_file ./models/RNN_v3b_256im_512z_1024h-vae_state_dict.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load VAE\n",
    "vae = VAE5(image_size=image_size, z_dim=128, conv_dim=64, code_dim=8, k_dim=512)\n",
    "if cuda:\n",
    "    vae.cuda()\n",
    "    \n",
    "# # Resume\n",
    "NAME='RNN_v3b_256im_512z_1024h'\n",
    "\n",
    "checkpoint_file = f'./models/RNN_v3b_256im_512z-joint-training_state_dict.pkl'\n",
    "if os.path.isfile(checkpoint_file):\n",
    "    state_dict = torch.load(checkpoint_file)\n",
    "    vae.load_state_dict(state_dict)\n",
    "    print(f'loaded checkpoint_file {checkpoint_file}')\n",
    "    \n",
    "save_file = f'./models/{NAME}-vae_state_dict.pkl'\n",
    "if os.path.isfile(save_file):\n",
    "    state_dict = torch.load(save_file)\n",
    "    vae.load_state_dict(state_dict)\n",
    "    print(f'loaded save_file {save_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T01:46:05.097703Z",
     "start_time": "2018-05-18T01:46:04.937269Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load MDRNN\n",
    "z_dim, action_dim, hidden_size, n_mixture, temp = 512, 12, 1024, 5, 0.0\n",
    "\n",
    "\n",
    "mdnrnn = MDNRNN(z_dim, action_dim, hidden_size, n_mixture, temp)\n",
    "\n",
    "if cuda:\n",
    "    mdnrnn = mdnrnn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-17T01:34:49.208458Z",
     "start_time": "2018-05-17T01:34:49.159052Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T01:46:05.517664Z",
     "start_time": "2018-05-18T01:46:05.099996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded ./models/RNN_v3b_256im_512z_1024h-mdnrnn_state_dict.pkl\n"
     ]
    }
   ],
   "source": [
    "# # Resume?\n",
    "save_file = f'./models/{NAME}-mdnrnn_state_dict.pkl'\n",
    "if os.path.isfile(save_file):\n",
    "    state_dict = torch.load(f'./models/{NAME}_state_dict.pkl')\n",
    "    mdnrnn.load_state_dict(state_dict)\n",
    "    print(f'loaded {save_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load inverse model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T01:46:05.536328Z",
     "start_time": "2018-05-18T01:46:05.519710Z"
    }
   },
   "outputs": [],
   "source": [
    "class FInv(torch.nn.modules.Module):\n",
    "    def __init__(self, z_dim, action_dim, hidden_size):\n",
    "        \"\"\"\n",
    "        Inverse model from https://arxiv.org/abs/1804.10689.\n",
    "        \n",
    "        Takes in z and z' and outputs predicted action\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.Linear(z_dim*2, hidden_size)\n",
    "        self.ln2 = nn.Linear(hidden_size,  action_dim)\n",
    "        \n",
    "    def forward(self, z_now, z_next):\n",
    "        x = torch.cat((z_now, z_next), dim=-1)\n",
    "        x = F.relu(self.ln1(x))\n",
    "        x = F.relu(self.ln2(x))\n",
    "        x = F.softmax(x, -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T01:46:05.587816Z",
     "start_time": "2018-05-18T01:46:05.570834Z"
    }
   },
   "outputs": [],
   "source": [
    "finv = FInv(z_dim, action_dim, hidden_size=256).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T01:46:06.032573Z",
     "start_time": "2018-05-18T01:46:05.961582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters 21877123\n",
      "Total trainable parameters 21877123\n",
      "Total parameters 14220800\n",
      "Total trainable parameters 14220800\n",
      "Total parameters 265484\n",
      "Total trainable parameters 265484\n"
     ]
    }
   ],
   "source": [
    "img = np.random.randn(image_size, image_size, 3)\n",
    "action = Variable(torch.from_numpy(np.random.randint(0,12,(12)))).float().cuda()[np.newaxis]\n",
    "gpu_img = Variable(torch.from_numpy(img[np.newaxis].transpose(0, 3, 1, 2))).float().cuda()\n",
    "if cuda:\n",
    "    gpu_img = gpu_img.cuda()\n",
    "with TorchSummarizeDf(vae) as tdf:\n",
    "    x, mu_vae, logvar_vae = vae.forward(gpu_img)\n",
    "    z = vae.sample(mu_vae, logvar_vae)\n",
    "    df_vae = tdf.make_df()\n",
    "#     loss_recon, loss_KLD = loss_function_vae(Y, x, mu_vae, sigma_vae)\n",
    "#     loss_vae = loss_recon + gamma * torch.abs(loss_KLD-C)\n",
    "#     loss_vae = loss_vae.mean() # mean along the batches\n",
    "with TorchSummarizeDf(mdnrnn) as tdf: \n",
    "    pi, mu, sigma, hidden_state = mdnrnn.forward(z.unsqueeze(1).repeat((1,2,1)), action.unsqueeze(1).repeat((1,2,1)))\n",
    "    z_next = mdnrnn.sample(pi, mu, sigma)\n",
    "#     loss_mdn = mdnrnn.rnn_loss(z, pi, mu, sigma).mean()\n",
    "    df_mdnrnn = tdf.make_df()\n",
    "\n",
    "#     loss = loss_mdn + gamma_vae * loss_vae\n",
    "with TorchSummarizeDf(finv) as tdf:\n",
    "    action_pred = finv(z.repeat((1,2,1)), z_next)\n",
    "        \n",
    "    df_finv = tdf.make_df()\n",
    "    \n",
    "del img, action, gpu_img, x, mu, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T01:46:06.107661Z",
     "start_time": "2018-05-18T01:46:06.034639Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>class_name</th>\n",
       "      <th>input_shape</th>\n",
       "      <th>output_shape</th>\n",
       "      <th>nb_params</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>encoder.0.conv</td>\n",
       "      <td>Conv2d</td>\n",
       "      <td>[(-1, 3, 256, 256)]</td>\n",
       "      <td>[(-1, 64, 256, 256)]</td>\n",
       "      <td>1792</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>encoder.0.bn</td>\n",
       "      <td>BatchNorm2d</td>\n",
       "      <td>[(-1, 64, 256, 256)]</td>\n",
       "      <td>[(-1, 64, 256, 256)]</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>encoder.0.act</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>[(-1, 64, 256, 256)]</td>\n",
       "      <td>[(-1, 64, 256, 256)]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>encoder.0</td>\n",
       "      <td>ConvBlock4</td>\n",
       "      <td>[(-1, 3, 256, 256)]</td>\n",
       "      <td>[(-1, 64, 256, 256)]</td>\n",
       "      <td>1920</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>encoder.1.conv</td>\n",
       "      <td>Conv2d</td>\n",
       "      <td>[(-1, 64, 256, 256)]</td>\n",
       "      <td>[(-1, 128, 128, 128)]</td>\n",
       "      <td>131200</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>encoder.1.bn</td>\n",
       "      <td>BatchNorm2d</td>\n",
       "      <td>[(-1, 128, 128, 128)]</td>\n",
       "      <td>[(-1, 128, 128, 128)]</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>encoder.1.act</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>[(-1, 128, 128, 128)]</td>\n",
       "      <td>[(-1, 128, 128, 128)]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>encoder.1</td>\n",
       "      <td>ConvBlock4</td>\n",
       "      <td>[(-1, 64, 256, 256)]</td>\n",
       "      <td>[(-1, 128, 128, 128)]</td>\n",
       "      <td>131456</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>encoder.2.conv</td>\n",
       "      <td>Conv2d</td>\n",
       "      <td>[(-1, 128, 128, 128)]</td>\n",
       "      <td>[(-1, 192, 64, 64)]</td>\n",
       "      <td>393408</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>encoder.2.bn</td>\n",
       "      <td>BatchNorm2d</td>\n",
       "      <td>[(-1, 192, 64, 64)]</td>\n",
       "      <td>[(-1, 192, 64, 64)]</td>\n",
       "      <td>384</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>encoder.2.act</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>[(-1, 192, 64, 64)]</td>\n",
       "      <td>[(-1, 192, 64, 64)]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>encoder.2</td>\n",
       "      <td>ConvBlock4</td>\n",
       "      <td>[(-1, 128, 128, 128)]</td>\n",
       "      <td>[(-1, 192, 64, 64)]</td>\n",
       "      <td>393792</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>encoder.3.conv</td>\n",
       "      <td>Conv2d</td>\n",
       "      <td>[(-1, 192, 64, 64)]</td>\n",
       "      <td>[(-1, 256, 32, 32)]</td>\n",
       "      <td>786688</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>encoder.3.bn</td>\n",
       "      <td>BatchNorm2d</td>\n",
       "      <td>[(-1, 256, 32, 32)]</td>\n",
       "      <td>[(-1, 256, 32, 32)]</td>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>encoder.3.act</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>[(-1, 256, 32, 32)]</td>\n",
       "      <td>[(-1, 256, 32, 32)]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>encoder.3</td>\n",
       "      <td>ConvBlock4</td>\n",
       "      <td>[(-1, 192, 64, 64)]</td>\n",
       "      <td>[(-1, 256, 32, 32)]</td>\n",
       "      <td>787200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>encoder.4.conv</td>\n",
       "      <td>Conv2d</td>\n",
       "      <td>[(-1, 256, 32, 32)]</td>\n",
       "      <td>[(-1, 320, 16, 16)]</td>\n",
       "      <td>1311040</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>encoder.4.bn</td>\n",
       "      <td>BatchNorm2d</td>\n",
       "      <td>[(-1, 320, 16, 16)]</td>\n",
       "      <td>[(-1, 320, 16, 16)]</td>\n",
       "      <td>640</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>encoder.4.act</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>[(-1, 320, 16, 16)]</td>\n",
       "      <td>[(-1, 320, 16, 16)]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>encoder.4</td>\n",
       "      <td>ConvBlock4</td>\n",
       "      <td>[(-1, 256, 32, 32)]</td>\n",
       "      <td>[(-1, 320, 16, 16)]</td>\n",
       "      <td>1311680</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>encoder.5.conv</td>\n",
       "      <td>Conv2d</td>\n",
       "      <td>[(-1, 320, 16, 16)]</td>\n",
       "      <td>[(-1, 384, 8, 8)]</td>\n",
       "      <td>1966464</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>encoder.5.bn</td>\n",
       "      <td>BatchNorm2d</td>\n",
       "      <td>[(-1, 384, 8, 8)]</td>\n",
       "      <td>[(-1, 384, 8, 8)]</td>\n",
       "      <td>768</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>encoder.5.act</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>[(-1, 384, 8, 8)]</td>\n",
       "      <td>[(-1, 384, 8, 8)]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>encoder.5</td>\n",
       "      <td>ConvBlock4</td>\n",
       "      <td>[(-1, 320, 16, 16)]</td>\n",
       "      <td>[(-1, 384, 8, 8)]</td>\n",
       "      <td>1967232</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>encoder.6</td>\n",
       "      <td>Conv2d</td>\n",
       "      <td>[(-1, 384, 8, 8)]</td>\n",
       "      <td>[(-1, 128, 8, 8)]</td>\n",
       "      <td>49280</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>mu</td>\n",
       "      <td>Linear</td>\n",
       "      <td>[(-1, 8192)]</td>\n",
       "      <td>[(-1, 512)]</td>\n",
       "      <td>4194816</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>logvar</td>\n",
       "      <td>Linear</td>\n",
       "      <td>[(-1, 8192)]</td>\n",
       "      <td>[(-1, 512)]</td>\n",
       "      <td>4194816</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>z</td>\n",
       "      <td>Linear</td>\n",
       "      <td>[(-1, 512)]</td>\n",
       "      <td>[(-1, 8192)]</td>\n",
       "      <td>4202496</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>decoder.0.deconv</td>\n",
       "      <td>ConvTranspose2d</td>\n",
       "      <td>[(-1, 128, 8, 8)]</td>\n",
       "      <td>[(-1, 384, 8, 8)]</td>\n",
       "      <td>49536</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>decoder.0.bn</td>\n",
       "      <td>BatchNorm2d</td>\n",
       "      <td>[(-1, 384, 8, 8)]</td>\n",
       "      <td>[(-1, 384, 8, 8)]</td>\n",
       "      <td>768</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>decoder.0.act</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>[(-1, 384, 8, 8)]</td>\n",
       "      <td>[(-1, 384, 8, 8)]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>decoder.0</td>\n",
       "      <td>DeconvBlock4</td>\n",
       "      <td>[(-1, 128, 8, 8)]</td>\n",
       "      <td>[(-1, 384, 8, 8)]</td>\n",
       "      <td>50304</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>decoder.1.deconv</td>\n",
       "      <td>ConvTranspose2d</td>\n",
       "      <td>[(-1, 384, 8, 8)]</td>\n",
       "      <td>[(-1, 320, 16, 16)]</td>\n",
       "      <td>1966400</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>decoder.1.bn</td>\n",
       "      <td>BatchNorm2d</td>\n",
       "      <td>[(-1, 320, 16, 16)]</td>\n",
       "      <td>[(-1, 320, 16, 16)]</td>\n",
       "      <td>640</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>decoder.1.act</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>[(-1, 320, 16, 16)]</td>\n",
       "      <td>[(-1, 320, 16, 16)]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>decoder.1</td>\n",
       "      <td>DeconvBlock4</td>\n",
       "      <td>[(-1, 384, 8, 8)]</td>\n",
       "      <td>[(-1, 320, 16, 16)]</td>\n",
       "      <td>1967040</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>decoder.2.deconv</td>\n",
       "      <td>ConvTranspose2d</td>\n",
       "      <td>[(-1, 320, 16, 16)]</td>\n",
       "      <td>[(-1, 256, 32, 32)]</td>\n",
       "      <td>1310976</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>decoder.2.bn</td>\n",
       "      <td>BatchNorm2d</td>\n",
       "      <td>[(-1, 256, 32, 32)]</td>\n",
       "      <td>[(-1, 256, 32, 32)]</td>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>decoder.2.act</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>[(-1, 256, 32, 32)]</td>\n",
       "      <td>[(-1, 256, 32, 32)]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>decoder.2</td>\n",
       "      <td>DeconvBlock4</td>\n",
       "      <td>[(-1, 320, 16, 16)]</td>\n",
       "      <td>[(-1, 256, 32, 32)]</td>\n",
       "      <td>1311488</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>decoder.3.deconv</td>\n",
       "      <td>ConvTranspose2d</td>\n",
       "      <td>[(-1, 256, 32, 32)]</td>\n",
       "      <td>[(-1, 192, 64, 64)]</td>\n",
       "      <td>786624</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>decoder.3.bn</td>\n",
       "      <td>BatchNorm2d</td>\n",
       "      <td>[(-1, 192, 64, 64)]</td>\n",
       "      <td>[(-1, 192, 64, 64)]</td>\n",
       "      <td>384</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>decoder.3.act</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>[(-1, 192, 64, 64)]</td>\n",
       "      <td>[(-1, 192, 64, 64)]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>decoder.3</td>\n",
       "      <td>DeconvBlock4</td>\n",
       "      <td>[(-1, 256, 32, 32)]</td>\n",
       "      <td>[(-1, 192, 64, 64)]</td>\n",
       "      <td>787008</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>decoder.4.deconv</td>\n",
       "      <td>ConvTranspose2d</td>\n",
       "      <td>[(-1, 192, 64, 64)]</td>\n",
       "      <td>[(-1, 128, 128, 128)]</td>\n",
       "      <td>393344</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>decoder.4.bn</td>\n",
       "      <td>BatchNorm2d</td>\n",
       "      <td>[(-1, 128, 128, 128)]</td>\n",
       "      <td>[(-1, 128, 128, 128)]</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>decoder.4.act</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>[(-1, 128, 128, 128)]</td>\n",
       "      <td>[(-1, 128, 128, 128)]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>decoder.4</td>\n",
       "      <td>DeconvBlock4</td>\n",
       "      <td>[(-1, 192, 64, 64)]</td>\n",
       "      <td>[(-1, 128, 128, 128)]</td>\n",
       "      <td>393600</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>decoder.5.deconv</td>\n",
       "      <td>ConvTranspose2d</td>\n",
       "      <td>[(-1, 128, 128, 128)]</td>\n",
       "      <td>[(-1, 64, 256, 256)]</td>\n",
       "      <td>131136</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>decoder.5.bn</td>\n",
       "      <td>BatchNorm2d</td>\n",
       "      <td>[(-1, 64, 256, 256)]</td>\n",
       "      <td>[(-1, 64, 256, 256)]</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>decoder.5.act</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>[(-1, 64, 256, 256)]</td>\n",
       "      <td>[(-1, 64, 256, 256)]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>decoder.5</td>\n",
       "      <td>DeconvBlock4</td>\n",
       "      <td>[(-1, 128, 128, 128)]</td>\n",
       "      <td>[(-1, 64, 256, 256)]</td>\n",
       "      <td>131264</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>decoder.6</td>\n",
       "      <td>Conv2d</td>\n",
       "      <td>[(-1, 64, 256, 256)]</td>\n",
       "      <td>[(-1, 3, 256, 256)]</td>\n",
       "      <td>1731</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>Sigmoid</td>\n",
       "      <td>[(-1, 3, 256, 256)]</td>\n",
       "      <td>[(-1, 3, 256, 256)]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name       class_name            input_shape  \\\n",
       "1     encoder.0.conv           Conv2d    [(-1, 3, 256, 256)]   \n",
       "2       encoder.0.bn      BatchNorm2d   [(-1, 64, 256, 256)]   \n",
       "3      encoder.0.act        LeakyReLU   [(-1, 64, 256, 256)]   \n",
       "4          encoder.0       ConvBlock4    [(-1, 3, 256, 256)]   \n",
       "5     encoder.1.conv           Conv2d   [(-1, 64, 256, 256)]   \n",
       "6       encoder.1.bn      BatchNorm2d  [(-1, 128, 128, 128)]   \n",
       "7      encoder.1.act        LeakyReLU  [(-1, 128, 128, 128)]   \n",
       "8          encoder.1       ConvBlock4   [(-1, 64, 256, 256)]   \n",
       "9     encoder.2.conv           Conv2d  [(-1, 128, 128, 128)]   \n",
       "10      encoder.2.bn      BatchNorm2d    [(-1, 192, 64, 64)]   \n",
       "11     encoder.2.act        LeakyReLU    [(-1, 192, 64, 64)]   \n",
       "12         encoder.2       ConvBlock4  [(-1, 128, 128, 128)]   \n",
       "13    encoder.3.conv           Conv2d    [(-1, 192, 64, 64)]   \n",
       "14      encoder.3.bn      BatchNorm2d    [(-1, 256, 32, 32)]   \n",
       "15     encoder.3.act        LeakyReLU    [(-1, 256, 32, 32)]   \n",
       "16         encoder.3       ConvBlock4    [(-1, 192, 64, 64)]   \n",
       "17    encoder.4.conv           Conv2d    [(-1, 256, 32, 32)]   \n",
       "18      encoder.4.bn      BatchNorm2d    [(-1, 320, 16, 16)]   \n",
       "19     encoder.4.act        LeakyReLU    [(-1, 320, 16, 16)]   \n",
       "20         encoder.4       ConvBlock4    [(-1, 256, 32, 32)]   \n",
       "21    encoder.5.conv           Conv2d    [(-1, 320, 16, 16)]   \n",
       "22      encoder.5.bn      BatchNorm2d      [(-1, 384, 8, 8)]   \n",
       "23     encoder.5.act        LeakyReLU      [(-1, 384, 8, 8)]   \n",
       "24         encoder.5       ConvBlock4    [(-1, 320, 16, 16)]   \n",
       "25         encoder.6           Conv2d      [(-1, 384, 8, 8)]   \n",
       "26                mu           Linear           [(-1, 8192)]   \n",
       "27            logvar           Linear           [(-1, 8192)]   \n",
       "28                 z           Linear            [(-1, 512)]   \n",
       "29  decoder.0.deconv  ConvTranspose2d      [(-1, 128, 8, 8)]   \n",
       "30      decoder.0.bn      BatchNorm2d      [(-1, 384, 8, 8)]   \n",
       "31     decoder.0.act        LeakyReLU      [(-1, 384, 8, 8)]   \n",
       "32         decoder.0     DeconvBlock4      [(-1, 128, 8, 8)]   \n",
       "33  decoder.1.deconv  ConvTranspose2d      [(-1, 384, 8, 8)]   \n",
       "34      decoder.1.bn      BatchNorm2d    [(-1, 320, 16, 16)]   \n",
       "35     decoder.1.act        LeakyReLU    [(-1, 320, 16, 16)]   \n",
       "36         decoder.1     DeconvBlock4      [(-1, 384, 8, 8)]   \n",
       "37  decoder.2.deconv  ConvTranspose2d    [(-1, 320, 16, 16)]   \n",
       "38      decoder.2.bn      BatchNorm2d    [(-1, 256, 32, 32)]   \n",
       "39     decoder.2.act        LeakyReLU    [(-1, 256, 32, 32)]   \n",
       "40         decoder.2     DeconvBlock4    [(-1, 320, 16, 16)]   \n",
       "41  decoder.3.deconv  ConvTranspose2d    [(-1, 256, 32, 32)]   \n",
       "42      decoder.3.bn      BatchNorm2d    [(-1, 192, 64, 64)]   \n",
       "43     decoder.3.act        LeakyReLU    [(-1, 192, 64, 64)]   \n",
       "44         decoder.3     DeconvBlock4    [(-1, 256, 32, 32)]   \n",
       "45  decoder.4.deconv  ConvTranspose2d    [(-1, 192, 64, 64)]   \n",
       "46      decoder.4.bn      BatchNorm2d  [(-1, 128, 128, 128)]   \n",
       "47     decoder.4.act        LeakyReLU  [(-1, 128, 128, 128)]   \n",
       "48         decoder.4     DeconvBlock4    [(-1, 192, 64, 64)]   \n",
       "49  decoder.5.deconv  ConvTranspose2d  [(-1, 128, 128, 128)]   \n",
       "50      decoder.5.bn      BatchNorm2d   [(-1, 64, 256, 256)]   \n",
       "51     decoder.5.act        LeakyReLU   [(-1, 64, 256, 256)]   \n",
       "52         decoder.5     DeconvBlock4  [(-1, 128, 128, 128)]   \n",
       "53         decoder.6           Conv2d   [(-1, 64, 256, 256)]   \n",
       "54           sigmoid          Sigmoid    [(-1, 3, 256, 256)]   \n",
       "\n",
       "             output_shape  nb_params  level  \n",
       "1    [(-1, 64, 256, 256)]       1792      2  \n",
       "2    [(-1, 64, 256, 256)]        128      2  \n",
       "3    [(-1, 64, 256, 256)]          0      2  \n",
       "4    [(-1, 64, 256, 256)]       1920      1  \n",
       "5   [(-1, 128, 128, 128)]     131200      2  \n",
       "6   [(-1, 128, 128, 128)]        256      2  \n",
       "7   [(-1, 128, 128, 128)]          0      2  \n",
       "8   [(-1, 128, 128, 128)]     131456      1  \n",
       "9     [(-1, 192, 64, 64)]     393408      2  \n",
       "10    [(-1, 192, 64, 64)]        384      2  \n",
       "11    [(-1, 192, 64, 64)]          0      2  \n",
       "12    [(-1, 192, 64, 64)]     393792      1  \n",
       "13    [(-1, 256, 32, 32)]     786688      2  \n",
       "14    [(-1, 256, 32, 32)]        512      2  \n",
       "15    [(-1, 256, 32, 32)]          0      2  \n",
       "16    [(-1, 256, 32, 32)]     787200      1  \n",
       "17    [(-1, 320, 16, 16)]    1311040      2  \n",
       "18    [(-1, 320, 16, 16)]        640      2  \n",
       "19    [(-1, 320, 16, 16)]          0      2  \n",
       "20    [(-1, 320, 16, 16)]    1311680      1  \n",
       "21      [(-1, 384, 8, 8)]    1966464      2  \n",
       "22      [(-1, 384, 8, 8)]        768      2  \n",
       "23      [(-1, 384, 8, 8)]          0      2  \n",
       "24      [(-1, 384, 8, 8)]    1967232      1  \n",
       "25      [(-1, 128, 8, 8)]      49280      1  \n",
       "26            [(-1, 512)]    4194816      0  \n",
       "27            [(-1, 512)]    4194816      0  \n",
       "28           [(-1, 8192)]    4202496      0  \n",
       "29      [(-1, 384, 8, 8)]      49536      2  \n",
       "30      [(-1, 384, 8, 8)]        768      2  \n",
       "31      [(-1, 384, 8, 8)]          0      2  \n",
       "32      [(-1, 384, 8, 8)]      50304      1  \n",
       "33    [(-1, 320, 16, 16)]    1966400      2  \n",
       "34    [(-1, 320, 16, 16)]        640      2  \n",
       "35    [(-1, 320, 16, 16)]          0      2  \n",
       "36    [(-1, 320, 16, 16)]    1967040      1  \n",
       "37    [(-1, 256, 32, 32)]    1310976      2  \n",
       "38    [(-1, 256, 32, 32)]        512      2  \n",
       "39    [(-1, 256, 32, 32)]          0      2  \n",
       "40    [(-1, 256, 32, 32)]    1311488      1  \n",
       "41    [(-1, 192, 64, 64)]     786624      2  \n",
       "42    [(-1, 192, 64, 64)]        384      2  \n",
       "43    [(-1, 192, 64, 64)]          0      2  \n",
       "44    [(-1, 192, 64, 64)]     787008      1  \n",
       "45  [(-1, 128, 128, 128)]     393344      2  \n",
       "46  [(-1, 128, 128, 128)]        256      2  \n",
       "47  [(-1, 128, 128, 128)]          0      2  \n",
       "48  [(-1, 128, 128, 128)]     393600      1  \n",
       "49   [(-1, 64, 256, 256)]     131136      2  \n",
       "50   [(-1, 64, 256, 256)]        128      2  \n",
       "51   [(-1, 64, 256, 256)]          0      2  \n",
       "52   [(-1, 64, 256, 256)]     131264      1  \n",
       "53    [(-1, 3, 256, 256)]       1731      1  \n",
       "54    [(-1, 3, 256, 256)]          0      0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vae[df_vae.level<3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T01:46:06.721332Z",
     "start_time": "2018-05-18T01:46:06.698362Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>class_name</th>\n",
       "      <th>input_shape</th>\n",
       "      <th>output_shape</th>\n",
       "      <th>nb_params</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ln1</td>\n",
       "      <td>Linear</td>\n",
       "      <td>[(-1, 2, 1024)]</td>\n",
       "      <td>[(-1, 2, 256)]</td>\n",
       "      <td>262400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ln2</td>\n",
       "      <td>Linear</td>\n",
       "      <td>[(-1, 2, 256)]</td>\n",
       "      <td>[(-1, 2, 12)]</td>\n",
       "      <td>3084</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name class_name      input_shape    output_shape  nb_params  level\n",
       "1  ln1     Linear  [(-1, 2, 1024)]  [(-1, 2, 256)]     262400      0\n",
       "2  ln2     Linear   [(-1, 2, 256)]   [(-1, 2, 12)]       3084      0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_finv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T01:46:06.747716Z",
     "start_time": "2018-05-18T01:46:06.723360Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>class_name</th>\n",
       "      <th>input_shape</th>\n",
       "      <th>output_shape</th>\n",
       "      <th>nb_params</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rnn</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>[[(-1, 2, 524)], [[(-1, 1, 1024)], [(-1, 1, 10...</td>\n",
       "      <td>[[(-1, 2, 1024)], [[(-1, 1, 1024)], [(-1, 1, 1...</td>\n",
       "      <td>6348800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mdn</td>\n",
       "      <td>Linear</td>\n",
       "      <td>[(-1, 1024), (-1, 1024)]</td>\n",
       "      <td>[(-1, 7680), (-1, 7680)]</td>\n",
       "      <td>7872000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name class_name                                        input_shape  \\\n",
       "1  rnn       LSTM  [[(-1, 2, 524)], [[(-1, 1, 1024)], [(-1, 1, 10...   \n",
       "2  mdn     Linear                           [(-1, 1024), (-1, 1024)]   \n",
       "\n",
       "                                        output_shape  nb_params  level  \n",
       "1  [[(-1, 2, 1024)], [[(-1, 1, 1024)], [(-1, 1, 1...    6348800      0  \n",
       "2                           [(-1, 7680), (-1, 7680)]    7872000      0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mdnrnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-17T02:41:36.949930Z",
     "start_time": "2018-05-17T02:02:24.572Z"
    }
   },
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T01:46:06.772085Z",
     "start_time": "2018-05-18T01:46:06.758859Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.modules.Module):\n",
    "    def __init__(self, vae, mdnrnn, finv):\n",
    "        super().__init__()\n",
    "        self.vae = vae\n",
    "        self.mdnrnn = mdnrnn\n",
    "        self.finv = finv\n",
    "        \n",
    "model = Model(vae, mdnrnn, finv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T01:46:06.880473Z",
     "start_time": "2018-05-18T01:46:06.865691Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "torch.save(finv.state_dict(), f'./models/{NAME}-finv_state_dict.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T01:46:08.541564Z",
     "start_time": "2018-05-18T01:46:08.525306Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim.lr_scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, verbose=True)\n",
    "\n",
    "# optimizer_vae = optim.Adam(vae.parameters(), lr=3e-5)\n",
    "# scheduler_vae = optim.lr_scheduler.ReduceLROnPlateau(optimizer_vae, mode='min', patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T15:16:21.354529Z",
     "start_time": "2018-05-10T15:16:21.337627Z"
    }
   },
   "source": [
    "# Train helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T01:46:09.766518Z",
     "start_time": "2018-05-18T01:46:09.742275Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Plot reconstructions\n",
    "def plot_results(loader, n=2, epoch=0, figsize=(9,6)):\n",
    "    vae.eval()\n",
    "    mdnrnn.eval()\n",
    "    \n",
    "    observations, actions, rewars, dones = next(iter(loader))\n",
    "    \n",
    "    X = Variable(observations.transpose(1,3))\n",
    "    _, channels, height, width = X.size()\n",
    "    if cuda:\n",
    "        X=X.cuda()\n",
    "    Y, mu_vae, logvar = vae.forward(X)\n",
    "    loss_recon, loss_KLD = loss_function_vae(Y, X, mu_vae, logvar)\n",
    "    loss_vae = loss_recon + gamma * torch.abs(loss_KLD-C)\n",
    "    \n",
    "    # TODO do we want to sample in test or training mode?\n",
    "    z_v = vae.sample(mu_vae, logvar)\n",
    "    \n",
    "    z_v = z_v.view(batch_size, seq_len, -1)\n",
    "    Y = Y.view((batch_size, seq_len, channels, height, width))\n",
    "    X = X.view((batch_size, seq_len, channels, height, width))\n",
    "    loss_vae = loss_vae.view(batch_size, seq_len, -1)\n",
    "    actions = actions.view(batch_size, seq_len, -1)\n",
    "    \n",
    "    # Forward\n",
    "    actions_v = Variable(actions).float()\n",
    "    \n",
    "\n",
    "    if cuda:\n",
    "        z_v=z_v.cuda()\n",
    "        actions_v=actions_v.cuda()\n",
    "    pi, mu, sigma, hidden_state = mdnrnn.forward(z_v, actions_v)\n",
    "\n",
    "    loss = mdnrnn.rnn_loss(z_v, pi, mu, sigma)\n",
    "    \n",
    "    mu = mu.mean(2).view((batch_size*seq_len, mdnrnn.z_dim))\n",
    "    X_pred = vae.decode(mu)\n",
    "    X_pred = X_pred.view((batch_size, seq_len, channels, height, width))\n",
    "    \n",
    "    # TODO finv    \n",
    "    \n",
    "    for i in np.linspace(0,seq_len-2,n):\n",
    "        batch = np.random.randint(0,batch_size)\n",
    "        i=int(i)\n",
    "        y=Y[batch][i].cpu().data.transpose(0,2).numpy()\n",
    "        x_orig = X[batch][i].transpose(0,2).data.cpu().numpy()\n",
    "        x_next = X[batch][i+1].transpose(0,2).data.cpu().numpy()\n",
    "        x_pred = X_pred[batch][i].transpose(0,2).data.cpu().numpy()\n",
    "        loss_vae_i = loss_vae[batch][i].cpu().data.item()\n",
    "        loss_i = loss[batch].cpu().data.item()\n",
    "        \n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        plt.subplot(2, 3, 1)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title('original')\n",
    "        plt.imshow(x_orig)\n",
    "\n",
    "        plt.subplot(2, 3, 4)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(y)\n",
    "        plt.title('reconstructed')\n",
    "           \n",
    "        plt.subplot(2, 3, 2)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(x_next)\n",
    "        plt.title('true next')\n",
    "        \n",
    "        plt.subplot(2, 3, 5)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(x_pred)\n",
    "        plt.title('pred next')\n",
    "        \n",
    "        plt.subplot(2, 3, 3)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(np.abs(x_orig-x_next))\n",
    "        plt.title('actual changes')\n",
    "        \n",
    "        plt.subplot(2, 3, 6)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(np.abs(y[i]-x_pred))\n",
    "        plt.title('predicted changes')\n",
    "\n",
    "        plt.suptitle('epoch {}, seq index {}, batch={}. vae_loss {:2.4f}, loss {:2.4f}'.format(\n",
    "            epoch, \n",
    "            i,\n",
    "            batch,\n",
    "            loss_vae_i, \n",
    "            loss_i\n",
    "        ))\n",
    "#         plt.subplots_adjust(wspace=-.4, hspace=.1)#, bottom=0.1, right=0.8, top=0.9)\n",
    "        plt.show()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T01:08:34.784962Z",
     "start_time": "2018-05-15T01:08:34.768271Z"
    }
   },
   "source": [
    "TODO\n",
    "\n",
    "- [ ] make a module containing all 3 including inverse model from https://arxiv.org/pdf/1804.10689.pdf\n",
    "    - that way they can use the same optimizer\n",
    "- [ ] do dual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T01:46:10.972792Z",
     "start_time": "2018-05-18T01:46:10.950432Z"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def train(loader, vae, mdnrnn, optimizer, max_batches=None, test=False, cuda=True, joint_training=False):\n",
    "    vae.eval()\n",
    "    if test:\n",
    "        mdnrnn.eval()\n",
    "    else:\n",
    "        mdnrnn.train()\n",
    "    info = collections.defaultdict(list)\n",
    "    hidden_state = None\n",
    "    if max_batches is None:\n",
    "        max_batches = len(loader)\n",
    "    else:\n",
    "        max_batches = min(max_batches, len(loader))\n",
    "    iterator = iter(loader)\n",
    "\n",
    "    with tqdm(total=max_batches*loader.batch_size, mininterval=0.5, desc='test' if test else 'training') as prog:\n",
    "        for i in range(max_batches):\n",
    "            # the loader batch_size is seq_len*batch_size\n",
    "            # we put it through the VAE as (seq_len*batch_size,...)\n",
    "            # then reshape to (batch_size,seq_len,...) for the mdnrnn\n",
    "            observations, actions, rewars, dones = next(iterator)\n",
    "            X = Variable(observations.transpose(1,3))\n",
    "            if cuda:\n",
    "                X=X.cuda()\n",
    "                \n",
    "            # VAE forward\n",
    "            Y, mu_vae, logvar = vae.forward(X)\n",
    "            # TODO do we want to sample in test or training mode?\n",
    "            z_v = vae.sample(mu_vae, logvar)\n",
    "            z_v = z_v.view(batch_size, seq_len, -1)\n",
    "            \n",
    "            loss_recon, loss_KLD = loss_function_vae(Y, X, mu_vae, logvar)\n",
    "            loss_vae = loss_recon + gamma * torch.abs(loss_KLD-C)\n",
    "            loss_vae = loss_vae.mean() # mean along the batches\n",
    "\n",
    "            # MDNRNN Forward\n",
    "            actions_v = Variable(actions).float()\n",
    "            actions_v = actions_v.view(batch_size, seq_len, -1)\n",
    "            if cuda:\n",
    "                z_v=z_v.cuda()\n",
    "                actions_v=actions_v.cuda()\n",
    "            pi, mu, sigma, hidden_state = mdnrnn.forward(z_v, actions_v)\n",
    "\n",
    "            loss_mdn = mdnrnn.rnn_loss(z_v, pi, mu, sigma).mean()\n",
    "            \n",
    "            # Finv forward\n",
    "            z_next = mdnrnn.sample(pi, mu, sigma)\n",
    "            z_now = z_v.unsqueeze(2).repeat((1, 1, mdnrnn.n_mixture, 1))\n",
    "            \n",
    "            action_pred = finv(z_v, z_next)\n",
    "            loss_inv = ((action_pred-actions_v)**2).sum(-1)\n",
    "            loss_inv = loss_inv.mean()\n",
    "            \n",
    "            loss = loss_mdn + lambda_vae * loss_vae + lambda_finv * loss_inv\n",
    "\n",
    "            if not test:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Record\n",
    "            info['loss_inv'].append(loss_inv.cpu().data.numpy())\n",
    "            info['loss_mdn'].append(loss_mdn.cpu().data.numpy())\n",
    "            info['loss_vae'].append(loss_vae.cpu().data.numpy())\n",
    "            info['loss_recon'].append(loss_recon.mean().cpu().data.item())\n",
    "            info['loss_KLD'].append(loss_KLD.mean().cpu().data.item())\n",
    "            \n",
    "            prog.update(loader.batch_size)\n",
    "            prog.desc='loss_rnn={loss_mdn:2.4f}, loss_inv= {lambda_finv}* {loss_inv:2.4f}, loss_vae={lambda_vae:2.4f} * ({loss_recon:2.2f} + {gamma}*|{loss_KLD:2.2f} - {C}|)'.format(\n",
    "                loss_mdn=np.mean(info['loss_mdn']), \n",
    "                loss_recon=np.mean(info['loss_recon']),\n",
    "                gamma=gamma,\n",
    "                lambda_finv=lambda_finv,\n",
    "                lambda_vae=lambda_vae,\n",
    "                C=C,\n",
    "                loss_KLD=np.mean(info['loss_KLD']),\n",
    "                loss_inv=np.mean(info['loss_inv'])\n",
    "            )\n",
    "            if i%400==0:\n",
    "                print('[{}/{}]'.format(i, max_batches), prog.desc)\n",
    "\n",
    "        print(prog.desc)\n",
    "        prog.close()\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-17T04:47:31.701105Z",
     "start_time": "2018-05-17T04:47:31.683884Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T01:46:12.773668Z",
     "start_time": "2018-05-18T01:46:12.744404Z"
    }
   },
   "outputs": [],
   "source": [
    "max_batches=40000//loader_train.batch_size\n",
    "max_batches\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T01:46:13.125996Z",
     "start_time": "2018-05-18T01:46:13.108967Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load previous history\n",
    "import pandas as pd\n",
    "if os.path.isfile(f'./models/{NAME}.csv'):\n",
    "    histories = pd.read_csv(f'./models/{NAME}.csv').to_dict(orient='records')\n",
    "else:\n",
    "    histories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-18T01:46:36.044Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eecb08651d474ced9742447b062c2c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='training', max=19188), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1066] loss_rnn=0.2933, loss_inv= 0.05* 2.8621, loss_vae=0.0001 * (2134.06 + 0.25*|65.76 - 0|)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/tqdm/_monitor.py:89: TqdmSynchronisationWarning: Set changed size during iteration (see https://github.com/tqdm/tqdm/issues/481)\n",
      "  TqdmSynchronisationWarning)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Run\n",
    "    info = train(loader_train, vae, mdnrnn, optimizer, max_batches=max_batches, test=False, cuda=True, joint_training=True)\n",
    "    torch.cuda.empty_cache()\n",
    "    info_val = train(loader_test, vae, mdnrnn, optimizer, max_batches=max_batches//6, test=True, cuda=True, joint_training=True)\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Adjust\n",
    "    scheduler.step(np.mean(info_val['loss_mdn']))\n",
    "    \n",
    "    # View\n",
    "    print('Epoch {}, loss={:2.4f}, loss_val={:2.4f}, loss_vae={:2.4f}, loss_vae_val={:2.4f},  loss_finv={:2.4f}, loss_finv_vae={:2.4f}, ,'.format(\n",
    "        epoch, \n",
    "        np.mean(info['loss_mdn']), \n",
    "        np.mean(info_val['loss_mdn']),\n",
    "        np.mean(info['loss_vae']), \n",
    "        np.mean(info_val['loss_vae']),\n",
    "        np.mean(info['loss_finv']),\n",
    "        np.mean(info_val['loss_finv'])\n",
    "    ))\n",
    "    plot_results(loader_test, n=2, epoch=epoch)\n",
    "    \n",
    "    # Record\n",
    "    history = {k+'_val':np.mean(v) for k,v in info_val.items()}\n",
    "    history.update({k:np.mean(v) for k,v in info.items()})\n",
    "    histories.append(history)\n",
    "    \n",
    "    torch.save(mdnrnn.state_dict(), f'./models/{NAME}-mdnrnn_{epoch}_state_dict.pkl')\n",
    "    torch.save(vae.state_dict(), f'./models/{NAME}-vae_{epoch}_state_dict.pkl')\n",
    "    torch.save(finv.state_dict(), f'./models/{NAME}-finv_{epoch}_state_dict.pkl')\n",
    "    \n",
    "    # Tidy\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T01:45:12.662191Z",
     "start_time": "2018-05-18T01:45:00.773Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_history = pd.DataFrame(histories)\n",
    "df_history.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T01:45:12.663622Z",
     "start_time": "2018-05-18T01:45:00.775Z"
    }
   },
   "outputs": [],
   "source": [
    "df_history[['loss_mdn','loss_mdn_val']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T01:45:12.664981Z",
     "start_time": "2018-05-18T01:45:00.778Z"
    }
   },
   "outputs": [],
   "source": [
    "df_history[['loss_vae','loss_vae_val']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T01:45:12.666357Z",
     "start_time": "2018-05-18T01:45:00.779Z"
    }
   },
   "outputs": [],
   "source": [
    "df_history[['loss_inv','loss_inv_val']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T01:45:12.667840Z",
     "start_time": "2018-05-18T01:45:00.782Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(mdnrnn.state_dict(), f'./models/{NAME}-mdnrnn_state_dict.pkl')\n",
    "torch.save(vae.state_dict(), f'./models/{NAME}-vae_state_dict.pkl')\n",
    "torch.save(finv.state_dict(), f'./models/{NAME}-finv_state_dict.pkl')\n",
    "df_history.to_csv(f'./models/{NAME}.csv', index=False)\n",
    "\n",
    "# torch.save(mdnrnn, f'./models/{NAME}-mdnrnn.pkl')\n",
    "# torch.save(vae, f'./models/{NAME}-vae')\n",
    "# torch.save(finv.state_dict(), f'./models/{NAME}-finv.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T04:22:01.737457Z",
     "start_time": "2018-05-15T04:21:54.572019Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T01:45:12.669262Z",
     "start_time": "2018-05-18T01:45:00.784Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_results(loader_test, n=4, epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T01:27:25.086326Z",
     "start_time": "2018-05-15T01:27:09.991863Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T01:45:12.670609Z",
     "start_time": "2018-05-18T01:45:00.788Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_results(loader_train, n=4, epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T01:45:12.672028Z",
     "start_time": "2018-05-18T01:45:00.790Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-11T02:44:27.867207Z",
     "start_time": "2018-05-11T02:44:27.849505Z"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "213px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
