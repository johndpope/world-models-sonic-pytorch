{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T15:16:12.961239Z",
     "start_time": "2018-05-10T15:16:12.955191Z"
    }
   },
   "source": [
    "TODO \n",
    "- [ ] use pytorch dists to samples and get density in order to make loss\n",
    "- [ ] check equations\n",
    "- [ ] train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-11T02:33:45.486834Z",
     "start_time": "2018-05-11T02:33:44.946818Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-11T02:33:48.495269Z",
     "start_time": "2018-05-11T02:33:45.489415Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "import torch.utils.data\n",
    "\n",
    "# load as dask array\n",
    "import dask.array as da\n",
    "import dask\n",
    "import h5py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-11T02:33:49.062016Z",
     "start_time": "2018-05-11T02:33:48.498448Z"
    }
   },
   "outputs": [],
   "source": [
    "from vae import VAE, loss_function\n",
    "from helpers.summarize import TorchSummarizeDf\n",
    "from helpers.dataset import NumpyDataset, TQDMDaskProgressBar, load_npzs\n",
    "from rnn import MDNRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T12:17:19.585326Z",
     "start_time": "2018-05-09T12:17:19.580159Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-11T02:33:50.021440Z",
     "start_time": "2018-05-11T02:33:49.064482Z"
    }
   },
   "outputs": [],
   "source": [
    "cuda= torch.cuda.is_available()\n",
    "env_name='sonic'\n",
    "num_epochs=200\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-11T02:33:58.777294Z",
     "start_time": "2018-05-11T02:33:50.024980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded ./models/VAE_2xv5_state_dict.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load VAE\n",
    "vae = VAE(image_size=128, z_dim=32, conv_dim=64, code_dim=16, k_dim=128)\n",
    "if cuda:\n",
    "    vae = vae.cuda()\n",
    "# # Resume?\n",
    "NAME='VAE_2xv5'\n",
    "save_file = f'./models/{NAME}_state_dict.pkl'\n",
    "if os.path.isfile(save_file):\n",
    "    state_dict = torch.load(save_file)\n",
    "    vae.load_state_dict(state_dict)\n",
    "    print(f'loaded {save_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-11T02:33:58.799556Z",
     "start_time": "2018-05-11T02:33:58.779754Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Test VAE\n",
    "# # load\n",
    "# data_cache_file = '/tmp/sonic_vae2.hdf5'\n",
    "# data = da.from_array(h5py.File(data_cache_file, mode='r')['x'], chunks=(2000, 128, 128, 3))\n",
    "# data\n",
    "# data_split = int(len(data)*0.8)\n",
    "# data_train = data[:data_split]\n",
    "# data_test = data[data_split:]\n",
    "# data_train, data_test\n",
    "\n",
    "   \n",
    "# dataset_train = NumpyDataset(data_train)\n",
    "# loader_train = torch.utils.data.DataLoader(dataset_train, pin_memory=True, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# dataset_test = NumpyDataset(data_test)\n",
    "# loader_test = torch.utils.data.DataLoader(dataset_test, pin_memory=True, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "# dataset_train, loader_train\n",
    "\n",
    "# # Plot reconstructions\n",
    "# def plot_results(loader=loader_test, n=2, epoch=0):\n",
    "#     x, = next(iter(loader))\n",
    "\n",
    "#     X = Variable(x).cuda().transpose(1,3).contiguous()\n",
    "#     Y, mu, logvar = vae.forward(X)\n",
    "#     loss = loss_function(Y, X, mu, logvar)\n",
    "\n",
    "#     y=Y.cpu().data.transpose(1,3).numpy()\n",
    "#     for i in range(n):\n",
    "#         plt.subplot(1, 2, 1)\n",
    "#         plt.title('original')\n",
    "#         plt.imshow(x[i].numpy())\n",
    "\n",
    "#         plt.subplot(1, 2, 2)\n",
    "#         plt.imshow(y[i])\n",
    "#         plt.title('reconstructed')\n",
    "\n",
    "#         plt.suptitle('epoch {}, index {}, loss {:2.4f}'.format(epoch, i, loss.cpu().data.numpy()))\n",
    "#         plt.show()\n",
    "        \n",
    "# plot_results(loader=loader_test, n=2, epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T15:16:21.354529Z",
     "start_time": "2018-05-10T15:16:21.337627Z"
    }
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-11T02:34:02.899521Z",
     "start_time": "2018-05-11T02:33:58.801847Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# take windows\n",
    "# make padded sequences, we need to make the data in shape (batch, window_of_timesteps, features)\n",
    "\n",
    "def timeseries_to_seq(x, window=3, pad=True):\n",
    "    \"\"\"\n",
    "  Inputs:\n",
    "  - x: shape (timeseries, features)\n",
    "  - window: e.g. 3\n",
    "  - pad: Wether to pad the input sequence or not (e.g. if you prepadded it or provided extra length)\n",
    "  Outputs:\n",
    "  - y: shape shape (batch, window, features)\n",
    "    where batch=timeseries-window if pad=False or batch=timeseries if pad=True\n",
    "    \n",
    "  Usage:\n",
    "  x= np.range(1000)\n",
    "  x_stacked = timeseries_to_seq(x, window=50)\n",
    "  m = torch.nn.LSTM()\n",
    "  y =  m(x_stacked)[0]\n",
    "  \"\"\"\n",
    "    if pad:\n",
    "        x = np.pad(x, [[window, 0], [0, 0]], mode='constant')\n",
    "    y = np.stack([x[i:i + window][::-1] for i in range(len(x)-window)], axis=1)\n",
    "    y = np.transpose(y, (1,0,2))\n",
    "    return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-11T01:42:08.644159Z",
     "start_time": "2018-05-11T01:42:08.563507Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-11T03:04:00.375650Z",
     "start_time": "2018-05-11T03:04:00.332249Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot reconstructions\n",
    "def plot_results(n=2, epoch=0, batch_size=6, figsize=(6,9)):\n",
    "    vae.eval()\n",
    "    mdnrnn.eval()\n",
    "    \n",
    "    # do a rollout\n",
    "    env = make_env('sonic')\n",
    "    try:\n",
    "        actions = []\n",
    "        observations = []\n",
    "        rewards=[]\n",
    "\n",
    "        observation = env.reset()\n",
    "        for i in range(batch_size):\n",
    "            action = env.action_space.sample()\n",
    "            observation, reward, done, env_info = env.step(action)\n",
    "            actions.append(action)\n",
    "            observations.append(observation)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                observation = env.reset()\n",
    "    except:\n",
    "        env.close()\n",
    "        raise\n",
    "    else:\n",
    "        env.close()\n",
    "\n",
    "    # stack\n",
    "    actions = np.stack(actions)\n",
    "    observations = np.stack(observations)\n",
    "    rewards = np.stack(rewards)\n",
    "\n",
    "    # Run VAE\n",
    "    X = Variable(torch.from_numpy(observations)).cuda().transpose(1,3).contiguous()\n",
    "    Y, mu, logvar = vae.forward(X)\n",
    "    z = vae.sample(mu, logvar).data.cpu().numpy()\n",
    "    loss_vae = loss_function(Y, X, mu, logvar)\n",
    "\n",
    "    # Stack into sequences\n",
    "    # TODO stack as torch vars\n",
    "    actions = timeseries_to_seq(actions, window=seq_len)\n",
    "    z = timeseries_to_seq(z, window=seq_len)\n",
    "\n",
    "    # Forward\n",
    "    z_v = Variable(torch.from_numpy(z)).cuda()\n",
    "    actions_v = Variable(torch.from_numpy(actions.astype(np.uint8))).float().cuda()\n",
    "    if cuda:\n",
    "        z_v=z_v.cuda()\n",
    "        actions_v=actions_v.cuda()\n",
    "    z_pred, hidden_state = mdnrnn.forward(z_v, actions_v)\n",
    "\n",
    "    loss = mdnrnn.rnn_loss(z_v, z_pred)\n",
    "\n",
    "    y=Y.cpu().data.transpose(1,3).numpy()\n",
    "    \n",
    "    X_pred = vae.decode(mu)\n",
    "    \n",
    "    for i in np.linspace(0,batch_size-2,n):\n",
    "        i=int(i)\n",
    "        x_orig = X[i].transpose(0,2).data.cpu().numpy()\n",
    "        x_next = X[i+1].transpose(0,2).data.cpu().numpy()\n",
    "        x_pred = X_pred[i].transpose(0,2).data.cpu().numpy()\n",
    "        loss_vae_i = loss_vae[i].cpu().data.item()\n",
    "        loss_i = loss[i].cpu().data.item()\n",
    "        \n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        plt.subplot(2, 3, 1)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title('original')\n",
    "        plt.imshow(x_orig)\n",
    "\n",
    "        plt.subplot(2, 3, 4)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(y[i])\n",
    "        plt.title('reconstructed')\n",
    "           \n",
    "        plt.subplot(2, 3, 2)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(x_next)\n",
    "        plt.title('true next')\n",
    "        \n",
    "        plt.subplot(2, 3, 5)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(x_pred)\n",
    "        plt.title('pred next')\n",
    "        \n",
    "        plt.subplot(2, 3, 3)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(np.abs(x_orig-x_next))\n",
    "        plt.title('actual changes')\n",
    "        \n",
    "        plt.subplot(2, 3, 6)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(np.abs(x_orig-x_pred))\n",
    "        plt.title('predicted changes')\n",
    "\n",
    "        plt.suptitle('epoch {}, index {}, vae_loss {:2.4f}, loss {:2.4f}'.format(epoch, i, loss_vae[i].cpu().data.item(), loss[i].cpu().data.item()))\n",
    "#         plt.subplots_adjust(wspace=-.4, hspace=.1)#, bottom=0.1, right=0.8, top=0.9)\n",
    "        plt.show()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-11T03:06:45.511073Z",
     "start_time": "2018-05-11T03:06:45.480203Z"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def train(make_env, vae, mdnrnn, optimizer, test=False, cuda=True, batch_size=30, num_batches=100):\n",
    "    try:\n",
    "        env=make_env('sonic')\n",
    "        vae.eval()\n",
    "        if test:\n",
    "            mdnrnn.eval()\n",
    "        else:\n",
    "            mdnrnn.train()\n",
    "        info = collections.defaultdict(list)\n",
    "\n",
    "        with tqdm(total=num_batches*batch_size, mininterval=0.5, desc='test' if test else 'training') as prog:\n",
    "            for i in range(num_batches):\n",
    "                # do a rollout\n",
    "                actions = []\n",
    "                observations = []\n",
    "                rewards=[]\n",
    "\n",
    "                observation = env.reset()\n",
    "                for i in range(batch_size):\n",
    "                    action = env.action_space.sample()\n",
    "                    observation, reward, done, env_info = env.step(action)\n",
    "                    actions.append(action)\n",
    "                    observations.append(observation)\n",
    "                    rewards.append(reward)\n",
    "                    if done:\n",
    "                        observation = env.reset()\n",
    "\n",
    "                # stack\n",
    "                actions = np.stack(actions)\n",
    "                observations = np.stack(observations)\n",
    "                rewards = np.stack(rewards)\n",
    "\n",
    "                # Run VAE\n",
    "                X = Variable(torch.from_numpy(observations)).cuda().transpose(1,3).contiguous()\n",
    "                Y, mu, logvar = vae.forward(X)\n",
    "                z = vae.sample(mu, logvar).data.cpu().numpy()\n",
    "#                 vae_loss = loss_function(Y, X, mu, logvar).mean()\n",
    "\n",
    "                # Stack into sequences\n",
    "                # TODO stack as torch vars\n",
    "                actions = timeseries_to_seq(actions, window=seq_len)\n",
    "                z = timeseries_to_seq(z, window=seq_len)\n",
    "\n",
    "                # Forward\n",
    "                z_v = Variable(torch.from_numpy(z))\n",
    "                actions_v = Variable(torch.from_numpy(actions.astype(np.uint8))).float()\n",
    "                if cuda:\n",
    "                    z_v=z_v.cuda()\n",
    "                    actions_v=actions_v.cuda()\n",
    "                z_pred, hidden_state = mdnrnn.forward(z_v, actions_v)\n",
    "\n",
    "                loss = mdnrnn.rnn_loss(z_v, z_pred).mean()\n",
    "                info['loss'].append(loss.cpu().data.numpy())\n",
    "\n",
    "                if not test:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                prog.update(batch_size)\n",
    "                prog.desc='loss={:2.4f}'.format(np.mean(info['loss']))\n",
    "\n",
    "            print('[{}/{}] loss={:2.4f}'.format(i, num_batches*batch_size, np.mean(info['loss'])))\n",
    "            prog.close()\n",
    "            env.close()\n",
    "    except:\n",
    "        env.close()\n",
    "        raise\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-11T03:08:16.687645Z",
     "start_time": "2018-05-11T03:08:16.656711Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load MDRNN\n",
    "z_dim, action_dim, hidden_size, n_mixture, temp = 128, 12, 256, 5, 0.0\n",
    "batch_size = 40\n",
    "seq_len = 4\n",
    "\n",
    "mdnrnn = MDNRNN(z_dim, action_dim, hidden_size, n_mixture, temp)\n",
    "\n",
    "if cuda:\n",
    "    mdnrnn = mdnrnn.cuda()\n",
    "\n",
    "optimizer = optim.Adam(mdnrnn.parameters(), lr=1e-5)\n",
    "import torch.optim.lr_scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-11T03:08:11.848Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3884e0d26f374e5d8fefb0c31db4f8f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='training', max=40000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/tqdm/_monitor.py:89: TqdmSynchronisationWarning: Set changed size during iteration (see https://github.com/tqdm/tqdm/issues/481)\n",
      "  TqdmSynchronisationWarning)\n"
     ]
    }
   ],
   "source": [
    "from custom_envs.env import make_env\n",
    "current_env_name= 'sonic'\n",
    "num_batches = 1000\n",
    "num_epochs = 200\n",
    "\n",
    "infos=[]\n",
    "for epoch in range(num_epochs):\n",
    "    info = train(make_env, vae, mdnrnn, optimizer, test=False, cuda=True, num_batches=num_batches, batch_size=batch_size)\n",
    "\n",
    "    info_val = train(make_env, vae, mdnrnn, optimizer, test=True, cuda=True)\n",
    "    scheduler.step(np.mean(info_val['loss']))\n",
    "    \n",
    "    print('Epoch {}, loss={:2.4f}, loss_val={:2.4f}'.format(epoch, np.mean(info['loss']), np.mean(info_val['loss'])))\n",
    "    infos.append([info, info_val])\n",
    "    \n",
    "    plot_results(n=2, epoch=epoch)\n",
    "    torch.save(vae.state_dict(), f'./models/{NAME}_{epoch}_state_dict.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-11T02:44:27.867207Z",
     "start_time": "2018-05-11T02:44:27.849505Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEBUG can we replace tf_normal with torch normal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-11T02:34:16.817180Z",
     "start_time": "2018-05-11T02:33:39.670Z"
    }
   },
   "outputs": [],
   "source": [
    "raise Exception('sd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-11T02:34:16.818738Z",
     "start_time": "2018-05-11T02:33:39.673Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DEBUG run one train in global scope\n",
    "from custom_envs.env import make_env\n",
    "current_env_name= 'sonic'\n",
    "batch_size = 20\n",
    "num_batches = 1000\n",
    "num_epochs = 20\n",
    "\n",
    "\n",
    "env = make_env(current_env_name)\n",
    "\n",
    "\n",
    "# do a rollout\n",
    "actions = []\n",
    "observations = []\n",
    "rewards=[]\n",
    "\n",
    "observation = env.reset()\n",
    "for i in range(batch_size):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    actions.append(action)\n",
    "    observations.append(observation)\n",
    "    rewards.append(reward)\n",
    "    if done:\n",
    "        observation = env.reset()\n",
    "\n",
    "# stack\n",
    "actions = np.stack(actions)\n",
    "observations = np.stack(observations)\n",
    "rewards = np.stack(rewards)\n",
    "#         actions.shape\n",
    "\n",
    "# Run VAE\n",
    "X = Variable(torch.from_numpy(observations)).cuda().transpose(1,3).contiguous()\n",
    "Y, mu, logvar = vae.forward(X)\n",
    "z = vae.sample(mu, logvar).data.cpu().numpy()\n",
    "#         z.shape\n",
    "\n",
    "# Stack into sequences\n",
    "actions = timeseries_to_seq(actions, window=seq_len)\n",
    "z = timeseries_to_seq(z, window=seq_len)\n",
    "\n",
    "# Forward\n",
    "z_v = Variable(torch.from_numpy(z)).cuda()\n",
    "actions_v = Variable(torch.from_numpy(actions.astype(np.uint8))).float().cuda()\n",
    "if cuda:\n",
    "    z_v=z_v.cuda()\n",
    "    actions_v=actions_v.cuda()\n",
    "z_pred, hidden_state = mdnrnn.forward(z_v, actions_v)\n",
    "\n",
    "loss = mdnrnn.rnn_loss(z_v, z_pred)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.sum().backward()\n",
    "optimizer.step()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-11T02:34:16.820301Z",
     "start_time": "2018-05-11T02:33:39.674Z"
    }
   },
   "outputs": [],
   "source": [
    "# DEBUG compare loss with https://github.com/hardmaru/pytorch_notebooks/blob/master/mixture_density_networks.ipynb\n",
    "oneDivSqrtTwoPI = 1.0 / np.sqrt(2.0*np.pi) # normalization factor for Gaussians\n",
    "def gaussian_distribution(y, mu, sigma):\n",
    "    # make |mu|=K copies of y, subtract mu, divide by sigma\n",
    "    expand_by=mu.size(-1)//y.size(-1)\n",
    "    y = y.repeat((1,1,expand_by))\n",
    "#     result = (y.expand_as(mu) - mu) * torch.reciprocal(sigma)\n",
    "    result = (y - mu) * 1/sigma\n",
    "    result = -0.5 * (result * result)\n",
    "    result = (torch.exp(result) /sigma) * oneDivSqrtTwoPI\n",
    "    result = result.view((result.size(0), result.size(1), expand_by, -1))\n",
    "    return result.sum(-1)\n",
    "\n",
    "def mdn_loss_fn(pi, sigma, mu, y):\n",
    "    result = gaussian_distribution(y, mu, sigma) \n",
    "    result = result * pi\n",
    "    result = torch.sum(result, dim=1)\n",
    "    result = -torch.log(result+1e-8)\n",
    "    return torch.mean(result,1)\n",
    "\n",
    "pi, mu, sigma = mdnrnn.get_mixture_coef(z_pred)\n",
    "loss2=mdn_loss_fn(pi, sigma, mu, z_v)\n",
    "\n",
    "loss1=mdnrnn.rnn_loss(z_v, z_pred)\n",
    "print(loss1.sum(),loss2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-11T02:34:16.821795Z",
     "start_time": "2018-05-11T02:33:39.675Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.distributions\n",
    "# replace tf_normal with pytorch normal dist logprob\n",
    "rollout_length = z_v.size(1)\n",
    "z_v2 = z_v.repeat((1,1,mdnrnn.n_mixture))\n",
    "mu2 = mu.view((-1, rollout_length, mdnrnn.n_mixture, mdnrnn.z_dim))\n",
    "sigma2 = sigma.view((-1, rollout_length, mdnrnn.n_mixture, mdnrnn.z_dim))\n",
    "z_v2 = z_v2.view(-1, rollout_length, mdnrnn.n_mixture, mdnrnn.z_dim)\n",
    "z_v2.shape, z_v.shape, mu.shape\n",
    "\n",
    "z_normals = torch.distributions.Normal(mu2, sigma2)\n",
    "z = z_normals.sample()\n",
    "z_prob = z_normals.log_prob(z_v2).exp()\n",
    "mu.shape, sigma.shape, z_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-11T02:34:16.823301Z",
     "start_time": "2018-05-11T02:33:39.677Z"
    }
   },
   "outputs": [],
   "source": [
    "z_v.shape, z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-11T02:34:16.824849Z",
     "start_time": "2018-05-11T02:33:39.679Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO confirm this works and the dimensions shapes are right\n",
    "from torch import normal, multinomial\n",
    "\n",
    "pi, mean, sigma = mdnrnn.get_mixture_coef(z_pred)\n",
    "\n",
    "batch_size, seq_len, z_dim  = z_pred.size(0)\n",
    "# randomly draw a mixture model\n",
    "\n",
    "# pi = pi.contiguous().view(-1), \n",
    "# mean = mean.contiguous().view(-1), \n",
    "# sigma = sigma.contiguous().view(-1)\n",
    "\n",
    "pi = pi.view(batch_size,-1)\n",
    "mean = mean.contiguous().view(batch_size,-1)\n",
    "sigma = sigma.contiguous().view(batch_size,-1)\n",
    "k = multinomial(pi, 1).long() # one for each batch\n",
    "\n",
    "# Multi d select\n",
    "start = (mdnrnn.z_dim * k).squeeze()\n",
    "end = (mdnrnn.z_dim * (k+1)).squeeze()\n",
    "index = torch.stack([torch.arange(start[i],end[i], dtype=torch.long) for i in range(len(start))])\n",
    "if cuda:\n",
    "    index=index.cuda()\n",
    "\n",
    "selected_mean = torch.stack([torch.index_select(mean[batch], index=index[batch], dim=0) for batch in range(batch_size)],0)\n",
    "\n",
    "selected_sigma = torch.stack([torch.index_select(sigma[batch], index=index[batch], dim=0) for batch in range(batch_size)],0)\n",
    "print(selected_mean.shape, selected_sigma.shape)\n",
    "\n",
    "z = normal(selected_mean, selected_sigma)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T01:34:16.126937Z",
     "start_time": "2018-05-10T01:34:16.096670Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
