{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiment, higher learning rate. Larger rollouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T23:15:07.013423Z",
     "start_time": "2018-06-05T23:15:05.777447Z"
    }
   },
   "outputs": [],
   "source": [
    "import deep_rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T23:15:07.051208Z",
     "start_time": "2018-06-05T23:15:07.016122Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T23:15:07.082394Z",
     "start_time": "2018-06-05T23:15:07.053641Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "import torch.utils.data\n",
    "\n",
    "# load as dask array\n",
    "import time\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T23:15:07.114488Z",
     "start_time": "2018-06-05T23:15:07.085363Z"
    }
   },
   "outputs": [],
   "source": [
    "from deep_rl.utils import Config\n",
    "from deep_rl.utils.logger import get_logger, get_default_log_dir\n",
    "\n",
    "from deep_rl.network.network_heads import CategoricalActorCriticNet, QuantileNet, OptionCriticNet, DeterministicActorCriticNet, GaussianActorCriticNet\n",
    "from deep_rl.network.network_bodies import FCBody\n",
    "from deep_rl.utils.normalizer import RunningStatsNormalizer\n",
    "from deep_rl.component.task import ParallelizedTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T23:15:07.495724Z",
     "start_time": "2018-06-05T23:15:07.116908Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing 0 potential games...\n",
      "Imported 0 games\n"
     ]
    }
   ],
   "source": [
    "from world_models_sonic.models.vae import VAE5, loss_function_vae\n",
    "from world_models_sonic.helpers.summarize import TorchSummarizeDf\n",
    "from world_models_sonic.models.rnn import MDNRNN2\n",
    "from world_models_sonic.models.inverse_model import InverseModel\n",
    "from world_models_sonic.models.world_model import WorldModel\n",
    "from world_models_sonic.custom_envs.env import make_env\n",
    "from world_models_sonic.custom_envs.wrappers import RandomGameReset\n",
    "from world_models_sonic import config\n",
    "from world_models_sonic.helpers.deep_rl import PPOAgent, run_iterations, SonicWorldModelDeepRL, CategoricalWorldActorCriticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T01:54:51.062733Z",
     "start_time": "2018-05-23T01:54:50.908162Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T07:15:35.605788Z",
     "start_time": "2018-05-20T07:15:35.588273Z"
    }
   },
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T23:15:07.530044Z",
     "start_time": "2018-06-05T23:15:07.499780Z"
    }
   },
   "outputs": [],
   "source": [
    "# #TODO make saves a hash of \n",
    "# agent_name, config.tag, agent.task.name\n",
    "# and timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T23:15:07.577472Z",
     "start_time": "2018-06-05T23:15:07.532778Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./outputs/RNN_v3b_128im_512z_1512_v6i_VAE5_all\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "env_name = 'sonic256'\n",
    "z_dim = 512  # latent dimensions\n",
    "channels = 3*4\n",
    "\n",
    "# RNN\n",
    "action_dim = 10\n",
    "image_size = 128\n",
    "\n",
    "verbose = True  # Set this true to render (and make it go slower)\n",
    "\n",
    "NAME = 'RNN_v3b_128im_512z_1512_v6i_VAE5_all'\n",
    "ppo_save_file = './outputs/{NAME}/PPO_512z_all_g.pkl'.format(NAME=NAME)\n",
    "\n",
    "if not os.path.isdir('./outputs/{NAME}'.format(NAME=NAME)):\n",
    "    os.makedirs('./outputs/{NAME}'.format(NAME=NAME))\n",
    "\n",
    "# Log to file and stream\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logger = logging.getLogger(NAME)\n",
    "\n",
    "log_dir = log_dir='./outputs/{NAME}'.format(NAME=NAME)\n",
    "print(log_dir)\n",
    "\n",
    "deep_rl_logger = get_logger(\n",
    "    NAME,\n",
    "    file_name='deep_rl_ppo.log',\n",
    "    level=logging.INFO,\n",
    "    log_dir='./outputs/{NAME}'.format(NAME=NAME), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T02:36:44.288963Z",
     "start_time": "2018-05-23T02:36:40.986Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T07:41:17.362954Z",
     "start_time": "2018-05-20T07:41:17.338009Z"
    }
   },
   "source": [
    "# World model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T23:15:13.213978Z",
     "start_time": "2018-06-05T23:15:07.580534Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load VAE\n",
    "# TODO swap z and k dim, since it's inconsistent with other models\n",
    "vae = VAE5(image_size=image_size, z_dim=128, conv_dim=64, code_dim=8, k_dim=z_dim, channels=channels)\n",
    "    \n",
    "# Load MDRNN\n",
    "action_dim, hidden_size, n_mixture, temp = action_dim, z_dim*2, 5, 0.0\n",
    "\n",
    "mdnrnn = MDNRNN2(z_dim, action_dim, hidden_size, n_mixture, temp)\n",
    "    \n",
    "finv = InverseModel(z_dim, action_dim, hidden_size=z_dim*2)\n",
    "    \n",
    "world_model = WorldModel(vae, mdnrnn, finv, logger=deep_rl_logger, lambda_vae_kld=1 / 1024., lambda_finv=1/100, lambda_vae=1, lambda_loss=1000)\n",
    "world_model = world_model.train()\n",
    "if cuda:\n",
    "    world_model = world_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T23:15:13.249320Z",
     "start_time": "2018-06-05T23:15:13.216390Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim.lr_scheduler\n",
    "torch.cuda.empty_cache()\n",
    "optimizer = optim.Adam(world_model.parameters(), lr=1e-5)\n",
    "\n",
    "world_model.optimizer = optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T07:18:55.625655Z",
     "start_time": "2018-05-20T07:18:55.606567Z"
    }
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T14:37:01.913659Z",
     "start_time": "2018-05-27T14:37:01.828185Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T23:15:15.477824Z",
     "start_time": "2018-06-05T23:15:13.251966Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game: SonicTheHedgehog2-Genesis state: ChemicalPlantZone.Act2\n",
      "game: SonicTheHedgehog2-Genesis state: WingFortressZone\n",
      "game: SonicAndKnuckles3-Genesis state: HiddenPalaceZone\n",
      "game: SonicTheHedgehog2-Genesis state: ChemicalPlantZone.Act1\n",
      "game: SonicTheHedgehog2-Genesis state: AquaticRuinZone.Act2\n",
      "game: SonicAndKnuckles3-Genesis state: IcecapZone.Act2\n",
      "game: SonicTheHedgehog2-Genesis state: AquaticRuinZone.Act1\n",
      "game: SonicTheHedgehog2-Genesis state: OilOceanZone.Act2\n",
      "rollout of  64\n",
      "mini batch 8\n",
      "loading ppo_save_file ./outputs/RNN_v3b_128im_512z_1512_v6i_VAE5_all/PPO_512z_all_g.pkl modified Wed Jun  6 07:14:54 2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ProcessWrapper-7:\n",
      "Process ProcessWrapper-5:\n",
      "Process ProcessWrapper-2:\n",
      "Process ProcessWrapper-8:\n",
      "Process ProcessWrapper-3:\n",
      "Process ProcessWrapper-1:\n",
      "Process ProcessWrapper-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wassname/.pyenv/versions/3.5.3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ProcessWrapper-6:\n",
      "  File \"/home/wassname/.pyenv/versions/3.5.3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/oldhome/wassname/Documents/projects/retro_sonic_comp/DeepRL/deep_rl/component/task.py\", line 177, in run\n",
      "    op, data = self.pipe.recv()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wassname/.pyenv/versions/3.5.3/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/wassname/.pyenv/versions/3.5.3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/wassname/.pyenv/versions/3.5.3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/media/oldhome/wassname/Documents/projects/retro_sonic_comp/DeepRL/deep_rl/component/task.py\", line 177, in run\n",
      "    op, data = self.pipe.recv()\n",
      "  File \"/media/oldhome/wassname/Documents/projects/retro_sonic_comp/DeepRL/deep_rl/component/task.py\", line 177, in run\n",
      "    op, data = self.pipe.recv()\n",
      "  File \"/media/oldhome/wassname/Documents/projects/retro_sonic_comp/DeepRL/deep_rl/component/task.py\", line 177, in run\n",
      "    op, data = self.pipe.recv()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wassname/.pyenv/versions/3.5.3/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/wassname/.pyenv/versions/3.5.3/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/wassname/.pyenv/versions/3.5.3/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/wassname/.pyenv/versions/3.5.3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/wassname/.pyenv/versions/3.5.3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/media/oldhome/wassname/Documents/projects/retro_sonic_comp/DeepRL/deep_rl/component/task.py\", line 177, in run\n",
      "    op, data = self.pipe.recv()\n",
      "  File \"/media/oldhome/wassname/Documents/projects/retro_sonic_comp/DeepRL/deep_rl/component/task.py\", line 177, in run\n",
      "    op, data = self.pipe.recv()\n",
      "  File \"/home/wassname/.pyenv/versions/3.5.3/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/wassname/.pyenv/versions/3.5.3/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/wassname/.pyenv/versions/3.5.3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/media/oldhome/wassname/Documents/projects/retro_sonic_comp/DeepRL/deep_rl/component/task.py\", line 177, in run\n",
      "    op, data = self.pipe.recv()\n",
      "  File \"/home/wassname/.pyenv/versions/3.5.3/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/wassname/.pyenv/versions/3.5.3/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/wassname/.pyenv/versions/3.5.3/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/wassname/.pyenv/versions/3.5.3/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/wassname/.pyenv/versions/3.5.3/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wassname/.pyenv/versions/3.5.3/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/wassname/.pyenv/versions/3.5.3/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/wassname/.pyenv/versions/3.5.3/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/wassname/.pyenv/versions/3.5.3/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/wassname/.pyenv/versions/3.5.3/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/wassname/.pyenv/versions/3.5.3/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/wassname/.pyenv/versions/3.5.3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/wassname/.pyenv/versions/3.5.3/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/wassname/.pyenv/versions/3.5.3/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/wassname/.pyenv/versions/3.5.3/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/wassname/.pyenv/versions/3.5.3/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/media/oldhome/wassname/Documents/projects/retro_sonic_comp/DeepRL/deep_rl/component/task.py\", line 177, in run\n",
      "    op, data = self.pipe.recv()\n",
      "  File \"/home/wassname/.pyenv/versions/3.5.3/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/wassname/.pyenv/versions/3.5.3/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/wassname/.pyenv/versions/3.5.3/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "z_state_dim=world_model.mdnrnn.z_dim + world_model.mdnrnn.hidden_size  + world_model.mdnrnn.action_dim\n",
    "\n",
    "\n",
    "def task_fn(log_dir):\n",
    "    return SonicWorldModelDeepRL(\n",
    "        env_fn=lambda: RandomGameReset(make_env(\n",
    "            'sonic', max_episode_steps=1000, to_gray=False, image_size=image_size)),\n",
    "        log_dir=log_dir,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "config = Config()\n",
    "\n",
    "verbose = False  # Set this true to render (and make it go slower)\n",
    "config.num_workers = 1 if verbose else 8\n",
    "config.task_fn = lambda: ParallelizedTask(\n",
    "    task_fn, config.num_workers, single_process=config.num_workers == 1)\n",
    "config.optimizer_fn = lambda params: torch.optim.RMSprop(params, 3e-4)\n",
    "config.network_fn = lambda state_dim, action_dim: CategoricalWorldActorCriticNet(\n",
    "    state_dim, action_dim, FCBody(z_state_dim, hidden_units=(64, 64), gate=F.relu), gpu=0 if cuda else -1, world_model_fn=lambda: world_model,\n",
    "    render=(config.num_workers==1 and verbose),\n",
    "    z_shape=(32, 16)\n",
    ")\n",
    "config.discount = 0.99\n",
    "config.logger = deep_rl_logger\n",
    "config.use_gae = True\n",
    "config.gae_tau = 0.95\n",
    "config.entropy_weight = 0.001\n",
    "config.gradient_clip = 0.4\n",
    "config.rollout_length = 1*64//config.num_workers\n",
    "config.optimization_epochs = 10\n",
    "config.num_mini_batches = 8*1\n",
    "config.ppo_ratio_clip = 0.2\n",
    "config.iteration_log_interval = 10\n",
    "\n",
    "# I tuned these so the intrinsic reward was 1) within an order of magnitude of the extrinsic. 2) smaller, 3) negative when stuck\n",
    "# TODO use reward normalisers to avoid the need for these hyperparameters\n",
    "config.curiosity_only = False\n",
    "config.curiosity_weight = 0.1\n",
    "config.curiosity_boredom = 0 # how many standard deviations above the mean does it's new experience need to be, so it's not bored\n",
    "config.intrinsic_reward_normalizer = RunningStatsNormalizer()\n",
    "config.reward_normalizer = RunningStatsNormalizer()\n",
    "agent = PPOAgent(config)\n",
    "\n",
    "print('rollout of ', config.rollout_length*config.num_workers)\n",
    "print('mini batch', (config.rollout_length*config.num_workers)//config.num_mini_batches)\n",
    "\n",
    "if os.path.isfile(ppo_save_file):\n",
    "    print('loading ppo_save_file', ppo_save_file, 'modified', time.ctime(os.path.getmtime(ppo_save_file)))\n",
    "    agent.load(ppo_save_file)\n",
    "    \n",
    "    # also load normalizers\n",
    "    state_dict = torch.load(ppo_save_file.replace('.pkl', '-intrinsic_reward_normalizer.pkl'))\n",
    "    config.intrinsic_reward_normalizer.load_state_dict(state_dict)\n",
    "\n",
    "    state_dict = torch.load(ppo_save_file.replace('.pkl', '-reward_normalizer.pkl'))\n",
    "    config.reward_normalizer.load_state_dict(state_dict)\n",
    "else:\n",
    "    print(\"couldn't find save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T23:15:15.526406Z",
     "start_time": "2018-06-05T23:15:15.481285Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticNet(\n",
       "  (phi_body): FCBody(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=1546, out_features=64, bias=True)\n",
       "      (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (actor_body): DummyBody()\n",
       "  (critic_body): DummyBody()\n",
       "  (fc_action): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (fc_critic): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.network.network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T23:15:15.560127Z",
     "start_time": "2018-06-05T23:15:15.529418Z"
    }
   },
   "outputs": [],
   "source": [
    "# # if we want to reset the actor\n",
    "# from deep_rl.network.network_heads import ActorCriticNet\n",
    "# agent.network.network = ActorCriticNet(agent.network.z_state_dim, action_dim, FCBody(z_state_dim, hidden_units=(64, 64), gate=F.relu), None, None)\n",
    "# agent.network.network.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T23:19:58.123463Z",
     "start_time": "2018-06-05T23:15:15.568757Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.0287/0.0287/0.0287, -0.0000/0.0000/-0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-06 07:15:19,562 - RNN_v3b_128im_512z_1512_v6i_VAE5_all - INFO: loss_rnn=35.9779, loss_inv= 7.4266=0.0100 * 742.6586, loss_vae=29.9502=1.0000 * (29.9279 + 0.0010 * 22.7637)\n",
      "2018-06-06 07:15:19,566 - RNN_v3b_128im_512z_1512_v6i_VAE5_all - INFO: total steps 64, min/mean/max reward 0.0000/0.0000/0.0000 of 8\n",
      "2018-06-06 07:15:19,570 - RNN_v3b_128im_512z_1512_v6i_VAE5_all - INFO: running min/mean/max reward 0.0000/0.0000/0.0000 of 8 0.4950 s/rollout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.0250/0.0250/0.0250, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.0861/0.0861/0.0861, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.1231/0.1231/0.1231, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.1164/0.1164/0.1164, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: -0.2051/-0.2051/-0.2051, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: -0.1996/-0.1996/-0.1996, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: -0.0785/-0.0785/-0.0785, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: -0.0421/-0.0421/-0.0421, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: -0.2103/-0.2103/-0.2103, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: -0.3021/-0.3021/-0.3021, -0.0000/0.0000/-0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-06 07:15:58,840 - RNN_v3b_128im_512z_1512_v6i_VAE5_all - INFO: loss_rnn=35.9779, loss_inv= 7.3698=0.0100 * 736.9768, loss_vae=28.2404=1.0000 * (28.2189 + 0.0010 * 22.0458)\n",
      "2018-06-06 07:15:58,841 - RNN_v3b_128im_512z_1512_v6i_VAE5_all - INFO: total steps 704, min/mean/max reward 0.0000/0.9397/5.1114 of 8\n",
      "2018-06-06 07:15:58,843 - RNN_v3b_128im_512z_1512_v6i_VAE5_all - INFO: running min/mean/max reward 0.0000/0.1401/5.1114 of 88 0.4913 s/rollout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout extrinsic, intrinsic reward [min/mean/max]: -0.0453/-0.0453/-0.0453, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: -0.1433/-0.1433/-0.1433, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.4131/0.4131/0.4131, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.0352/0.0352/0.0352, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.1648/0.1648/0.1648, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.4260/0.4260/0.4260, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.4758/0.4758/0.4758, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: -0.1531/-0.1531/-0.1531, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.1012/0.1012/0.1012, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.1218/0.1218/0.1218, -0.0000/0.0000/-0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-06 07:16:35,826 - RNN_v3b_128im_512z_1512_v6i_VAE5_all - INFO: loss_rnn=35.9776, loss_inv= 7.4028=0.0100 * 740.2777, loss_vae=27.9973=1.0000 * (27.9758 + 0.0010 * 22.0457)\n",
      "2018-06-06 07:16:35,827 - RNN_v3b_128im_512z_1512_v6i_VAE5_all - INFO: total steps 1344, min/mean/max reward 0.0000/0.9397/5.1114 of 8\n",
      "2018-06-06 07:16:35,831 - RNN_v3b_128im_512z_1512_v6i_VAE5_all - INFO: running min/mean/max reward 0.0000/0.5209/5.1114 of 168 0.4775 s/rollout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout extrinsic, intrinsic reward [min/mean/max]: -0.1903/-0.1903/-0.1903, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.0945/0.0945/0.0945, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.1447/0.1447/0.1447, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.1569/0.1569/0.1569, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: -0.1811/-0.1811/-0.1811, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.4098/0.4098/0.4098, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: -0.1342/-0.1342/-0.1342, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: -0.2515/-0.2515/-0.2515, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: -0.2644/-0.2644/-0.2644, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: -0.1497/-0.1497/-0.1497, -0.0000/0.0000/-0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-06 07:17:14,467 - RNN_v3b_128im_512z_1512_v6i_VAE5_all - INFO: loss_rnn=35.9777, loss_inv= 7.3701=0.0100 * 737.0135, loss_vae=27.6704=1.0000 * (27.6490 + 0.0010 * 21.8947)\n",
      "2018-06-06 07:17:14,468 - RNN_v3b_128im_512z_1512_v6i_VAE5_all - INFO: total steps 1984, min/mean/max reward 0.0000/0.9397/5.1114 of 8\n",
      "2018-06-06 07:17:14,476 - RNN_v3b_128im_512z_1512_v6i_VAE5_all - INFO: running min/mean/max reward 0.0000/0.6560/5.1114 of 248 0.4793 s/rollout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.3261/0.3261/0.3261, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.6927/0.6927/0.6927, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.2823/0.2823/0.2823, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.0124/0.0124/0.0124, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: -0.0278/-0.0278/-0.0278, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.0098/0.0098/0.0098, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.0420/0.0420/0.0420, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.2584/0.2584/0.2584, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.4391/0.4391/0.4391, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.7386/0.7386/0.7386, -0.0000/0.0000/-0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-06 07:17:52,207 - RNN_v3b_128im_512z_1512_v6i_VAE5_all - INFO: loss_rnn=35.9777, loss_inv= 7.3595=0.0100 * 735.9513, loss_vae=27.4282=1.0000 * (27.4068 + 0.0010 * 21.9552)\n",
      "2018-06-06 07:17:52,209 - RNN_v3b_128im_512z_1512_v6i_VAE5_all - INFO: total steps 2624, min/mean/max reward 1.6981/6.8708/13.7001 of 8\n",
      "2018-06-06 07:17:52,210 - RNN_v3b_128im_512z_1512_v6i_VAE5_all - INFO: running min/mean/max reward 0.0000/2.2051/16.0815 of 328 0.4775 s/rollout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.7757/0.7757/0.7757, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.5904/0.5904/0.5904, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.4027/0.4027/0.4027, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.1298/0.1298/0.1298, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: -0.0913/-0.0913/-0.0913, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.0717/0.0717/0.0717, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: -0.0027/-0.0027/-0.0027, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: -0.0687/-0.0687/-0.0687, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: -0.1318/-0.1318/-0.1318, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.3691/0.3691/0.3691, -0.0000/0.0000/-0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-06 07:18:29,648 - RNN_v3b_128im_512z_1512_v6i_VAE5_all - INFO: loss_rnn=35.9777, loss_inv= 7.3310=0.0100 * 733.0998, loss_vae=27.4590=1.0000 * (27.4374 + 0.0010 * 22.1388)\n",
      "2018-06-06 07:18:29,649 - RNN_v3b_128im_512z_1512_v6i_VAE5_all - INFO: total steps 3264, min/mean/max reward 0.7896/7.9297/23.2307 of 8\n",
      "2018-06-06 07:18:29,650 - RNN_v3b_128im_512z_1512_v6i_VAE5_all - INFO: running min/mean/max reward 0.0000/3.2277/23.2307 of 408 0.4756 s/rollout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.3729/0.3729/0.3729, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: -0.2044/-0.2044/-0.2044, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: -0.1986/-0.1986/-0.1986, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: -0.1871/-0.1871/-0.1871, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: -0.1793/-0.1793/-0.1793, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.0722/0.0722/0.0722, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.3556/0.3556/0.3556, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: -0.0450/-0.0450/-0.0450, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: -0.2374/-0.2374/-0.2374, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: -0.1734/-0.1734/-0.1734, -0.0000/0.0000/-0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-06 07:19:06,793 - RNN_v3b_128im_512z_1512_v6i_VAE5_all - INFO: loss_rnn=35.9777, loss_inv= 7.3139=0.0100 * 731.3881, loss_vae=27.5882=1.0000 * (27.5662 + 0.0010 * 22.5347)\n",
      "2018-06-06 07:19:06,794 - RNN_v3b_128im_512z_1512_v6i_VAE5_all - INFO: total steps 3904, min/mean/max reward 2.2185/8.7887/23.2307 of 8\n",
      "2018-06-06 07:19:06,795 - RNN_v3b_128im_512z_1512_v6i_VAE5_all - INFO: running min/mean/max reward 0.0000/4.1145/23.2307 of 488 0.4737 s/rollout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout extrinsic, intrinsic reward [min/mean/max]: -0.2302/-0.2302/-0.2302, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.0774/0.0774/0.0774, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.3486/0.3486/0.3486, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.1410/0.1410/0.1410, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.0873/0.0873/0.0873, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.4504/0.4504/0.4504, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.6999/0.6999/0.6999, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.6864/0.6864/0.6864, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.4327/0.4327/0.4327, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.5012/0.5012/0.5012, -0.0000/0.0000/-0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-06 07:19:43,520 - RNN_v3b_128im_512z_1512_v6i_VAE5_all - INFO: loss_rnn=35.9778, loss_inv= 7.3210=0.0100 * 732.0953, loss_vae=28.0035=1.0000 * (27.9816 + 0.0010 * 22.4887)\n",
      "2018-06-06 07:19:43,521 - RNN_v3b_128im_512z_1512_v6i_VAE5_all - INFO: total steps 4544, min/mean/max reward 1.9422/7.9349/20.4118 of 8\n",
      "2018-06-06 07:19:43,523 - RNN_v3b_128im_512z_1512_v6i_VAE5_all - INFO: running min/mean/max reward 0.0000/5.3381/23.2307 of 500 0.4717 s/rollout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.0914/0.0914/0.0914, -0.0000/0.0000/-0.0000\n",
      "rollout extrinsic, intrinsic reward [min/mean/max]: 0.3282/0.3282/0.3282, -0.0000/0.0000/-0.0000\n",
      "saving ./outputs/RNN_v3b_128im_512z_1512_v6i_VAE5_all/PPO_512z_all_g.pkl\n",
      "saving backup ./outputs/RNN_v3b_128im_512z_1512_v6i_VAE5_all/PPO_512z_all_g-20180605_23-19-57.pkl\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-bd0a1904a78b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mrun_iterations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/oldhome/wassname/Documents/projects/retro_sonic_comp/world-models-pytorch/world_models_sonic/helpers/deep_rl/utils.py\u001b[0m in \u001b[0;36mrun_iterations\u001b[0;34m(agent, log_dir)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'steps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rewards'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_episode_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/oldhome/wassname/Documents/projects/retro_sonic_comp/world-models-pytorch/world_models_sonic/helpers/deep_rl/ppo_agent.py\u001b[0m in \u001b[0;36miteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0msampled_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minitial_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_world_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_next_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_hidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_world_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_next_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mintrinsic_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_loss\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/oldhome/wassname/Documents/projects/retro_sonic_comp/world-models-pytorch/world_models_sonic/helpers/deep_rl/network_heads.py\u001b[0m in \u001b[0;36mtrain_world_model\u001b[0;34m(self, obs, action, next_obs, hidden_state, train)\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mnext_obs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mhidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m             )\n\u001b[1;32m    103\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/oldhome/wassname/Documents/projects/retro_sonic_comp/world-models-pytorch/world_models_sonic/models/world_model.py\u001b[0m in \u001b[0;36mforward_train\u001b[0;34m(self, X, actions, X_next, hidden_state, test)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mz_obs_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz_obs_next\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdnrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# We are evaluating how the output distribution for the next step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/oldhome/wassname/Documents/projects/retro_sonic_comp/world-models-pytorch/world_models_sonic/models/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inpt, action_discrete, hidden_state)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0maction_discrete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# one hot code the action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction_discrete\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mcuda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cuda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    run_iterations(agent, log_dir=log_dir)\n",
    "except:\n",
    "    if config.num_workers == 1:\n",
    "        agent.task.tasks[0].env.close()\n",
    "    else:\n",
    "        [t.close() for t in agent.task.tasks]\n",
    "    print(\"saving\", ppo_save_file)\n",
    "    agent.save(ppo_save_file)\n",
    "    torch.save(config.intrinsic_reward_normalizer.state_dict(), ppo_save_file.replace('.pkl', '-intrinsic_reward_normalizer.pkl'))\n",
    "    torch.save(config.reward_normalizer.state_dict(), ppo_save_file.replace('.pkl', '-reward_normalizer.pkl'))\n",
    "\n",
    "    # Backup since it sometimes get's corrupted\n",
    "    ts = datetime.datetime.utcnow().strftime('%Y%m%d_%H-%M-%S')\n",
    "    print(\"saving backup\",\n",
    "          ppo_save_file.replace('.pkl', '-%s.pkl' % ts),)\n",
    "    agent.save(ppo_save_file.replace('.pkl', '-%s.pkl' % ts))\n",
    "    # TODO save and load normalizers\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T23:19:58.124642Z",
     "start_time": "2018-06-05T23:15:05.493Z"
    }
   },
   "outputs": [],
   "source": [
    "agent.save(ppo_save_file)\n",
    "torch.save(config.intrinsic_reward_normalizer.state_dict(), ppo_save_file.replace('.pkl', '-intrinsic_reward_normalizer.pkl'))\n",
    "torch.save(config.reward_normalizer.state_dict(), ppo_save_file.replace('.pkl', '-reward_normalizer.pkl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:16:00.803289Z",
     "start_time": "2018-06-05T12:16:00.773246Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-22T13:30:58.458184Z",
     "start_time": "2018-05-22T13:30:58.382867Z"
    }
   },
   "source": [
    "\n",
    "To monitor with tensorboard\n",
    "```sh\n",
    "cd ~/Documents/projects/retro_sonic_comp/world-models-pytorch/log \n",
    "tensorboard  --logdir .\n",
    "#then open http://localhost:6006/#scalars\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T23:19:58.125899Z",
     "start_time": "2018-06-05T23:15:05.501Z"
    }
   },
   "outputs": [],
   "source": [
    "agent.save(ppo_save_file)\n",
    "ppo_save_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T08:02:13.543439Z",
     "start_time": "2018-05-20T08:02:13.516675Z"
    }
   },
   "source": [
    "# summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T23:19:58.127153Z",
     "start_time": "2018-06-05T23:15:05.506Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "with torch.no_grad():\n",
    "    img = np.random.randn(image_size, image_size, 3)\n",
    "    action = np.array(np.random.randint(0,action_dim))[np.newaxis]\n",
    "    action = Variable(torch.from_numpy(action)).float().cuda()[np.newaxis]\n",
    "    gpu_img = Variable(torch.from_numpy(img[np.newaxis].transpose(0, 3, 1, 2))).float().cuda()\n",
    "    if cuda:\n",
    "        gpu_img = gpu_img.cuda()\n",
    "    with TorchSummarizeDf(vae) as tdf:\n",
    "        x, mu_vae, logvar_vae = vae.forward(gpu_img)\n",
    "        z = vae.sample(mu_vae, logvar_vae)\n",
    "        df_vae = tdf.make_df()\n",
    "\n",
    "    display(df_vae[df_vae.level<2])\n",
    "    \n",
    "    with TorchSummarizeDf(mdnrnn) as tdf: \n",
    "        pi, mu, sigma, hidden_state = mdnrnn.forward(z.unsqueeze(1).repeat((1,2,1)))\n",
    "        z_next = mdnrnn.sample(pi, mu, sigma)\n",
    "        df_mdnrnn = tdf.make_df()\n",
    "    \n",
    "    display(df_mdnrnn)\n",
    "    \n",
    "\n",
    "    with TorchSummarizeDf(finv) as tdf:\n",
    "        finv(z.repeat((1,2,1)), z_next)   \n",
    "        df_finv = tdf.make_df()\n",
    "    display(df_finv)\n",
    "\n",
    "    with TorchSummarizeDf(world_model) as tdf:\n",
    "        world_model(gpu_img, action)\n",
    "        df_world_model = tdf.make_df()\n",
    "    display(df_world_model[df_world_model.level<2])\n",
    "    \n",
    "    del img, action, gpu_img, x, mu, z, z_next, mu_vae, pi, sigma, logvar_vae"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter3",
   "language": "python",
   "name": "jupyter3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "notify_time": "5",
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "185px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "613px",
    "left": "0px",
    "right": "20px",
    "top": "138px",
    "width": "158px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
