{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiment, higher learning rate. Larger rollouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T10:39:53.304544Z",
     "start_time": "2018-06-05T10:39:52.143187Z"
    }
   },
   "outputs": [],
   "source": [
    "import deep_rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T10:39:53.344646Z",
     "start_time": "2018-06-05T10:39:53.307819Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T10:39:53.388165Z",
     "start_time": "2018-06-05T10:39:53.350213Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "import torch.utils.data\n",
    "\n",
    "# load as dask array\n",
    "import time\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T10:39:53.418582Z",
     "start_time": "2018-06-05T10:39:53.390649Z"
    }
   },
   "outputs": [],
   "source": [
    "from deep_rl.utils import Config\n",
    "from deep_rl.utils.logger import get_logger, get_default_log_dir\n",
    "\n",
    "from deep_rl.network.network_heads import CategoricalActorCriticNet, QuantileNet, OptionCriticNet, DeterministicActorCriticNet, GaussianActorCriticNet\n",
    "from deep_rl.network.network_bodies import FCBody\n",
    "\n",
    "from deep_rl.component.task import ParallelizedTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T10:39:53.783145Z",
     "start_time": "2018-06-05T10:39:53.420770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing 0 potential games...\n",
      "Imported 0 games\n"
     ]
    }
   ],
   "source": [
    "from world_models_sonic.models.vae import VAE5, loss_function_vae\n",
    "from world_models_sonic.helpers.summarize import TorchSummarizeDf\n",
    "from world_models_sonic.models.rnn import MDNRNN2\n",
    "from world_models_sonic.models.inverse_model import InverseModel\n",
    "from world_models_sonic.models.world_model import WorldModel\n",
    "from world_models_sonic.custom_envs.env import make_env\n",
    "from world_models_sonic.custom_envs.wrappers import RandomGameReset\n",
    "from world_models_sonic import config\n",
    "from world_models_sonic.helpers.deep_rl import PPOAgent, run_iterations, SonicWorldModelDeepRL, CategoricalWorldActorCriticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T01:54:51.062733Z",
     "start_time": "2018-05-23T01:54:50.908162Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T07:15:35.605788Z",
     "start_time": "2018-05-20T07:15:35.588273Z"
    }
   },
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T10:39:53.815738Z",
     "start_time": "2018-06-05T10:39:53.785409Z"
    }
   },
   "outputs": [],
   "source": [
    "# #TODO make saves a hash of \n",
    "# agent_name, config.tag, agent.task.name\n",
    "# and timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T10:39:53.865081Z",
     "start_time": "2018-06-05T10:39:53.818674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./outputs/RNN_v3b_128im_512z_1512_v6i_VAE5_all\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "env_name = 'sonic256'\n",
    "z_dim = 512  # latent dimensions\n",
    "channels = 3*4\n",
    "\n",
    "# RNN\n",
    "action_dim = 10\n",
    "image_size = 128\n",
    "\n",
    "verbose = True  # Set this true to render (and make it go slower)\n",
    "\n",
    "NAME = 'RNN_v3b_128im_512z_1512_v6i_VAE5_all'\n",
    "ppo_save_file = './outputs/{NAME}/PPO_512z_all_g.pkl'.format(NAME=NAME)\n",
    "\n",
    "if not os.path.isdir('./outputs/{NAME}'.format(NAME=NAME)):\n",
    "    os.makedirs('./outputs/{NAME}'.format(NAME=NAME))\n",
    "\n",
    "# Log to file and stream\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logger = logging.getLogger(NAME)\n",
    "\n",
    "log_dir = log_dir='./outputs/{NAME}'.format(NAME=NAME)\n",
    "print(log_dir)\n",
    "\n",
    "deep_rl_logger = get_logger(\n",
    "    NAME,\n",
    "    file_name='deep_rl_ppo.log',\n",
    "    level=logging.INFO,\n",
    "    log_dir='./outputs/{NAME}'.format(NAME=NAME), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T02:36:44.288963Z",
     "start_time": "2018-05-23T02:36:40.986Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T07:41:17.362954Z",
     "start_time": "2018-05-20T07:41:17.338009Z"
    }
   },
   "source": [
    "# World model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T10:39:59.270592Z",
     "start_time": "2018-06-05T10:39:53.867844Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load VAE\n",
    "# TODO swap z and k dim, since it's inconsistent with other models\n",
    "vae = VAE5(image_size=image_size, z_dim=128, conv_dim=64, code_dim=8, k_dim=z_dim, channels=channels)\n",
    "    \n",
    "# Load MDRNN\n",
    "action_dim, hidden_size, n_mixture, temp = action_dim, z_dim*2, 5, 0.0\n",
    "\n",
    "mdnrnn = MDNRNN2(z_dim, action_dim, hidden_size, n_mixture, temp)\n",
    "    \n",
    "finv = InverseModel(z_dim, action_dim, hidden_size=z_dim*2)\n",
    "    \n",
    "world_model = WorldModel(vae, mdnrnn, finv, logger=deep_rl_logger, lambda_vae_kld=1 / 1024., lambda_finv=1/100, lambda_vae=3, lambda_loss=1000)\n",
    "world_model = world_model.train()\n",
    "if cuda:\n",
    "    world_model = world_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T10:39:59.304782Z",
     "start_time": "2018-06-05T10:39:59.273131Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim.lr_scheduler\n",
    "torch.cuda.empty_cache()\n",
    "optimizer = optim.Adam(world_model.parameters(), lr=1e-4)\n",
    "\n",
    "world_model.optimizer = optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T07:18:55.625655Z",
     "start_time": "2018-05-20T07:18:55.606567Z"
    }
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T14:37:01.913659Z",
     "start_time": "2018-05-27T14:37:01.828185Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T10:39:59.834477Z",
     "start_time": "2018-06-05T10:39:59.307111Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game: SonicTheHedgehog-Genesis state: GreenHillZone.Act3\n",
      "rollout of  256\n",
      "mini batch 8\n",
      "loading ppo_save_file ./outputs/RNN_v3b_128im_512z_1512_v6i_VAE5_all/PPO_512z_all_g.pkl modified Tue Jun  5 18:39:27 2018\n"
     ]
    }
   ],
   "source": [
    "z_state_dim=world_model.mdnrnn.z_dim + world_model.mdnrnn.hidden_size  + world_model.mdnrnn.action_dim\n",
    "\n",
    "\n",
    "def task_fn(log_dir):\n",
    "    return SonicWorldModelDeepRL(\n",
    "        env_fn=lambda: RandomGameReset(make_env(\n",
    "            'sonic', max_episode_steps=1000, to_gray=False, image_size=image_size)),\n",
    "        log_dir=log_dir,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "config = Config()\n",
    "\n",
    "verbose = True  # Set this true to render (and make it go slower)\n",
    "config.num_workers = 1 if verbose else 8\n",
    "config.task_fn = lambda: ParallelizedTask(\n",
    "    task_fn, config.num_workers, single_process=config.num_workers == 1)\n",
    "config.optimizer_fn = lambda params: torch.optim.RMSprop(params, 3e-4)\n",
    "config.network_fn = lambda state_dim, action_dim: CategoricalWorldActorCriticNet(\n",
    "    state_dim, action_dim, FCBody(z_state_dim, hidden_units=(64, 64), gate=F.relu), gpu=0 if cuda else -1, world_model_fn=lambda: world_model,\n",
    "    render=(config.num_workers==1 and verbose),\n",
    "    z_shape=(32, 16)\n",
    ")\n",
    "config.discount = 0.99\n",
    "config.logger = deep_rl_logger\n",
    "config.use_gae = True\n",
    "config.gae_tau = 0.95\n",
    "config.entropy_weight = 0.001\n",
    "config.gradient_clip = 0.4\n",
    "config.rollout_length = 2*64//config.num_workers\n",
    "config.optimization_epochs = 10\n",
    "config.num_mini_batches = 8*2\n",
    "config.ppo_ratio_clip = 0.2\n",
    "config.iteration_log_interval = 10\n",
    "\n",
    "# I tuned these so the intrinsic reward was 1) within an order of magnitude of the extrinsic. 2) smaller, 3) negative when stuck\n",
    "# TODO use reward normalisers to avoid the need for these hyperparameters\n",
    "config.curiosity_only = False\n",
    "config.curiosity_weight = 0.03\n",
    "config.curiosity_baseline = 3\n",
    "agent = PPOAgent(config)\n",
    "\n",
    "print('rollout of ', config.rollout_length*config.num_workers)\n",
    "print('mini batch', (config.rollout_length*config.num_workers)//config.num_mini_batches)\n",
    "\n",
    "if os.path.isfile(ppo_save_file):\n",
    "    print('loading ppo_save_file', ppo_save_file, 'modified', time.ctime(os.path.getmtime(ppo_save_file)))\n",
    "    agent.load(ppo_save_file)\n",
    "else:\n",
    "    print(\"couldn't find save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T10:39:59.867190Z",
     "start_time": "2018-06-05T10:39:59.836854Z"
    }
   },
   "outputs": [],
   "source": [
    "# # if we want to reset the actor\n",
    "# from deep_rl.network.network_heads import ActorCriticNet\n",
    "# agent.network.network = ActorCriticNet(agent.network.z_state_dim, action_dim, FCBody(z_state_dim, hidden_units=(64, 64), gate=F.relu), None, None)\n",
    "# agent.network.network.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T10:41:52.575997Z",
     "start_time": "2018-06-05T10:39:59.869534Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout extrinsic vs intrinsic reward 0.0121 -0.0017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-05 18:40:39,758 - RNN_v3b_128im_512z_1512_v6i_VAE5_all - INFO: loss_rnn=35.9779, loss_inv= 6.8016=0.0100 * 680.1586, loss_vae=20.6082=3.0000 * (6.8505 + 0.0010 * 19.3781)\n",
      "2018-06-05 18:40:39,760 - RNN_v3b_128im_512z_1512_v6i_VAE5_all - INFO: total steps 256, min/mean/max reward 2.3912/2.3912/2.3912 of 1\n",
      "2018-06-05 18:40:39,761 - RNN_v3b_128im_512z_1512_v6i_VAE5_all - INFO: running min/mean/max reward 2.3912/2.3912/2.3912 of 1 39.8574 s/rollout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout extrinsic vs intrinsic reward 0.0781 -0.0016\n",
      "saving ./outputs/RNN_v3b_128im_512z_1512_v6i_VAE5_all/PPO_512z_all_g.pkl\n",
      "saving backup ./outputs/RNN_v3b_128im_512z_1512_v6i_VAE5_all/PPO_512z_all_g-20180605_10-41-51.pkl\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /pytorch/aten/src/THC/generic/THCStorage.cu:58",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-46d32d7c665a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mrun_iterations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/oldhome/wassname/Documents/projects/retro_sonic_comp/world-models-pytorch/world_models_sonic/helpers/deep_rl/utils.py\u001b[0m in \u001b[0;36mrun_iterations\u001b[0;34m(agent, log_dir)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'steps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rewards'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_episode_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/oldhome/wassname/Documents/projects/retro_sonic_comp/world-models-pytorch/world_models_sonic/helpers/deep_rl/ppo_agent.py\u001b[0m in \u001b[0;36miteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0msampled_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_reduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_next_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_hidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;31m# Update reward in the rollout, using reducing in loss: curiosity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/oldhome/wassname/Documents/projects/retro_sonic_comp/world-models-pytorch/world_models_sonic/helpers/deep_rl/network_heads.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, obs, action, next_obs, hidden_state, model_train)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mis_rollout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_obs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mobs_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_reduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# Predict next action and value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/oldhome/wassname/Documents/projects/retro_sonic_comp/world-models-pytorch/world_models_sonic/helpers/deep_rl/network_heads.py\u001b[0m in \u001b[0;36mprocess_obs\u001b[0;34m(self, obs, action, next_obs, hidden_state, model_train)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mnext_obs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0mhidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             )\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/oldhome/wassname/Documents/projects/retro_sonic_comp/world-models-pytorch/world_models_sonic/models/world_model.py\u001b[0m in \u001b[0;36mforward_train\u001b[0;34m(self, X, actions, X_next, hidden_state, test)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# TODO ideally should pass the losses back to the agent and do logging and backprop there\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.3/envs/jupyter3/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.3/envs/jupyter3/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /pytorch/aten/src/THC/generic/THCStorage.cu:58"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    run_iterations(agent, log_dir=log_dir)\n",
    "except:\n",
    "    if config.num_workers == 1:\n",
    "        agent.task.tasks[0].env.close()\n",
    "    else:\n",
    "        [t.close() for t in agent.task.tasks]\n",
    "    print(\"saving\", ppo_save_file)\n",
    "    agent.save(ppo_save_file)\n",
    "\n",
    "    # Backup since it sometimes get's corrupted\n",
    "    ts = datetime.datetime.utcnow().strftime('%Y%m%d_%H-%M-%S')\n",
    "    print(\"saving backup\",\n",
    "          ppo_save_file.replace('.pkl', '-%s.pkl' % ts),)\n",
    "    agent.save(ppo_save_file.replace('.pkl', '-%s.pkl' % ts))\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T06:37:42.409895Z",
     "start_time": "2018-06-05T06:37:40.723Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-05T08:48:12.309Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-22T13:30:58.458184Z",
     "start_time": "2018-05-22T13:30:58.382867Z"
    }
   },
   "source": [
    "\n",
    "To monitor with tensorboard\n",
    "```sh\n",
    "cd ~/Documents/projects/retro_sonic_comp/world-models-pytorch/log \n",
    "tensorboard  --logdir .\n",
    "#then open http://localhost:6006/#scalars\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T10:41:52.577213Z",
     "start_time": "2018-06-05T10:39:51.884Z"
    }
   },
   "outputs": [],
   "source": [
    "agent.save(ppo_save_file)\n",
    "ppo_save_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T08:02:13.543439Z",
     "start_time": "2018-05-20T08:02:13.516675Z"
    }
   },
   "source": [
    "# summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T10:41:52.578302Z",
     "start_time": "2018-06-05T10:39:51.887Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "with torch.no_grad():\n",
    "    img = np.random.randn(image_size, image_size, 3)\n",
    "    action = np.array(np.random.randint(0,action_dim))[np.newaxis]\n",
    "    action = Variable(torch.from_numpy(action)).float().cuda()[np.newaxis]\n",
    "    gpu_img = Variable(torch.from_numpy(img[np.newaxis].transpose(0, 3, 1, 2))).float().cuda()\n",
    "    if cuda:\n",
    "        gpu_img = gpu_img.cuda()\n",
    "    with TorchSummarizeDf(vae) as tdf:\n",
    "        x, mu_vae, logvar_vae = vae.forward(gpu_img)\n",
    "        z = vae.sample(mu_vae, logvar_vae)\n",
    "        df_vae = tdf.make_df()\n",
    "\n",
    "    display(df_vae[df_vae.level<2])\n",
    "    \n",
    "    with TorchSummarizeDf(mdnrnn) as tdf: \n",
    "        pi, mu, sigma, hidden_state = mdnrnn.forward(z.unsqueeze(1).repeat((1,2,1)))\n",
    "        z_next = mdnrnn.sample(pi, mu, sigma)\n",
    "        df_mdnrnn = tdf.make_df()\n",
    "    \n",
    "    display(df_mdnrnn)\n",
    "    \n",
    "\n",
    "    with TorchSummarizeDf(finv) as tdf:\n",
    "        finv(z.repeat((1,2,1)), z_next)   \n",
    "        df_finv = tdf.make_df()\n",
    "    display(df_finv)\n",
    "\n",
    "    with TorchSummarizeDf(world_model) as tdf:\n",
    "        world_model(gpu_img, action)\n",
    "        df_world_model = tdf.make_df()\n",
    "    display(df_world_model[df_world_model.level<2])\n",
    "    \n",
    "    del img, action, gpu_img, x, mu, z, z_next, mu_vae, pi, sigma, logvar_vae"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter3",
   "language": "python",
   "name": "jupyter3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "notify_time": "5",
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "185px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "613px",
    "left": "0px",
    "right": "20px",
    "top": "138px",
    "width": "158px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
