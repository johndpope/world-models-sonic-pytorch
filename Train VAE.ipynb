{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "If it stalls around loss 60, you might need a smaller batch size, e.g. 8\n",
    "\n",
    "\n",
    "\n",
    "## Log\n",
    "\n",
    "### 20180506\n",
    "Why is this not working? It stalls around loss=60 and just reconstructs the same mean image each time.\n",
    "\n",
    "- batchnorm and pad? no lots of other models use them\n",
    "    - https://github.com/josephsmann/UnsupervisedDeepLearning-Pytorch/blob/jm/udlp/autoencoder/convVAE.py#L19\n",
    "    - https://github.com/taey16/pix2pixBEGAN.pytorch/blob/master/models/BEGAN.py\n",
    "- relu, same\n",
    "- lr - similar to other models\n",
    "- inner params - similar but larger than other working models\n",
    "- batch? Maybe it seems better with a lower batch (e.g. 8 instead of 32)\n",
    "- loss, this seems right\n",
    "    - except some people use a loss balance https://github.com/AppliedDataSciencePartners/WorldModels/blob/master/vae/arch.py#L97\n",
    "        where they multuply reconstruction loss by 10 but i've got ~100 vs ~1e-4 so I don't really need to!\n",
    "        \n",
    "- ok a lower batch size seemed to get it over that initial hump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T02:38:44.157975Z",
     "start_time": "2018-05-07T02:38:44.154955Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T02:38:47.630607Z",
     "start_time": "2018-05-07T02:38:44.164961Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-06T00:21:02.585280Z",
     "start_time": "2018-05-06T00:21:02.474215Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T02:38:50.824665Z",
     "start_time": "2018-05-07T02:38:47.633102Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "import torch.utils.data\n",
    "\n",
    "# load as dask array\n",
    "import dask.array as da\n",
    "import dask\n",
    "import h5py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T02:38:53.970740Z",
     "start_time": "2018-05-07T02:38:50.827400Z"
    }
   },
   "outputs": [],
   "source": [
    "from vae import VAE, loss_function\n",
    "from helpers.summarize import TorchSummarizeDf\n",
    "from helpers.dataset import NumpyDataset, TQDMDaskProgressBar, load_npzs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T02:38:56.942566Z",
     "start_time": "2018-05-07T02:38:53.973456Z"
    }
   },
   "outputs": [],
   "source": [
    "env_name='sonic'\n",
    "cuda= torch.cuda.is_available()\n",
    "num_epochs=200\n",
    "batch_size=6\n",
    "data_cache_file = '/tmp/sonic_vae2.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-07T02:38:42.742Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 128, 128, 3)\n",
      "dask.array<concatenate, shape=(120000, 128, 128, 3), dtype=float32, chunksize=(6000, 128, 128, 3)>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20df39b0016d4e9a9a92413dccb4afc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=61), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load as dask array\n",
    "\n",
    "filenames = sorted(glob.glob('./data/vae/obs_data_' + env_name + '_*.npz'))\n",
    "\n",
    "if not os.path.isfile(data_cache_file):\n",
    "    data_train = load_npzs(filenames)\n",
    "    print(data_train)\n",
    "    with TQDMDaskProgressBar():\n",
    "        da.to_hdf5(data_cache_file, '/x', data_train)\n",
    "       \n",
    "    # clear mem\n",
    "    del data_train \n",
    "    import gc\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-06T07:11:28.978952Z",
     "start_time": "2018-05-06T07:11:28.772281Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-07T02:38:42.747Z"
    }
   },
   "outputs": [],
   "source": [
    "# load\n",
    "data = da.from_array(h5py.File(data_cache_file)['x'], chunks=(2000, 128, 128, 3))\n",
    "data\n",
    "data_split = int(len(data)*0.8)\n",
    "data_train = data[:data_split]\n",
    "data_test = data[data_split:]\n",
    "data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-07T02:38:42.749Z"
    }
   },
   "outputs": [],
   "source": [
    "   \n",
    "dataset_train = NumpyDataset(data_train)\n",
    "loader_train = torch.utils.data.DataLoader(dataset_train, pin_memory=True, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "\n",
    "dataset_test = NumpyDataset(data_test)\n",
    "loader_test = torch.utils.data.DataLoader(dataset_test, pin_memory=True, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "dataset_train, loader_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-06T01:28:34.496506Z",
     "start_time": "2018-05-06T01:28:27.517Z"
    }
   },
   "source": [
    "# View model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-07T02:38:42.751Z"
    }
   },
   "outputs": [],
   "source": [
    "vae = VAE(image_size=128, z_dim=32, conv_dim=64, code_dim=8, k_dim=128).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-07T02:38:42.754Z"
    }
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load('./models/VAE_state_dict.pkl')\n",
    "vae.load_state_dict(vae.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-07T02:38:42.757Z"
    }
   },
   "outputs": [],
   "source": [
    "img = np.random.randn(64, 64, 3)\n",
    "img = np.random.randn(64*2, 64*2, 3)\n",
    "gpu_img = Variable(torch.from_numpy(img[np.newaxis].transpose(0, 3, 1, 2))).float().cuda()\n",
    "\n",
    "with TorchSummarizeDf(vae) as tdf:\n",
    "    x, mu, logvar = vae.forward(gpu_img)\n",
    "    print(x.size())\n",
    "    print(loss_function(x, gpu_img, mu, logvar))\n",
    "    x = x.data.cpu().numpy()[0].transpose(1, 2, 0)\n",
    "    df = tdf.make_df()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-07T02:38:42.759Z"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def train(loader, net, optimizer, loss_function, test=False, cuda=True):\n",
    "    if test:\n",
    "        net.eval()\n",
    "    else:\n",
    "        net.train()\n",
    "    info = collections.defaultdict(list)\n",
    "    \n",
    "    with tqdm(total=len(loader)*loader.batch_size, mininterval=0.5, desc='test' if test else 'training') as prog:\n",
    "        for i, (batch,) in enumerate(loader):\n",
    "            x = Variable(batch.transpose(1,3)).cuda() #*255 # FIXME (I divided by 255 once too many during gathering)\n",
    "            y, mu, logvar = vae.forward(x)\n",
    "            loss = loss_function(y, x, mu, logvar)\n",
    "            info['loss'].append(loss.cpu().data.numpy()[0])\n",
    "            \n",
    "            if not test:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            prog.update(loader.batch_size)\n",
    "            prog.desc='loss={:2.4f}'.format(np.mean(info['loss'][-100:]))\n",
    "            \n",
    "            if i%(100000//batch_size)==0:\n",
    "                print('[{}/{}] loss={:2.4f}'.format(i, len(loader), np.mean(info['loss'][-100:])))\n",
    "        print('[{}/{}] loss={:2.4f}'.format(i, len(loader), np.mean(info['loss'][-100:])))\n",
    "        prog.close()\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-07T02:38:42.762Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot reconstructions\n",
    "def plot_results(loader=loader_test, n=2, epoch=0):\n",
    "    x, = next(iter(loader))\n",
    "\n",
    "    X = Variable(x).cuda().transpose(1,3).contiguous()\n",
    "    Y, mu, logvar = vae.forward(X)\n",
    "    loss = loss_function(Y, X, mu, logvar)\n",
    "\n",
    "    y=Y.cpu().data.transpose(1,3).numpy()\n",
    "    for i in range(n):\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title('original')\n",
    "        plt.imshow(x[i].numpy())\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(y[i])\n",
    "        plt.title('reconstructed, loss {:2.4f}'.format(loss.cpu().data.numpy()[0]))\n",
    "\n",
    "        plt.suptitle('epoch {}, index {}, original'.format(epoch, i))\n",
    "        plt.show()\n",
    "        \n",
    "# plot_results(loader=loader_test, n=2, epoch=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-07T02:38:42.764Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "import torch.optim.lr_scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=6, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-07T02:38:42.766Z"
    }
   },
   "outputs": [],
   "source": [
    "infos=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-07T02:38:42.768Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs): \n",
    "    info = train(loader_train, vae, optimizer, loss_function, test=False, cuda=True)\n",
    "    info_val = train(loader_test, vae, optimizer, loss_function, test=True, cuda=True)\n",
    "    scheduler.step(np.mean(info_val['loss']))\n",
    "    \n",
    "    print('Epoch {}, loss={:2.4f}, loss_val={:2.4f}'.format(epoch, np.mean(info['loss']), np.mean(info_val['loss'])))\n",
    "    infos.append([info, info_val])\n",
    "    \n",
    "    plot_results(loader=loader_test, n=2, epoch=epoch)\n",
    "    \n",
    "    torch.save(vae, './models/VAE_{}.pkl'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-07T02:38:42.771Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(vae, './models/VAE.pkl')\n",
    "torch.save(vae.state_dict(), './models/VAE_state_dict.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-07T02:38:42.773Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot history\n",
    "histories = []\n",
    "for info, info_val in infos:\n",
    "    history = {k+'_val':np.mean(v) for k,v in info_val.items()}\n",
    "    history.update({k:np.mean(v) for k,v in info.items()})\n",
    "    histories.append(history)\n",
    "histories = pd.DataFrame(histories)\n",
    "histories.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-07T02:38:42.775Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot reconstructions\n",
    "x, = next(iter(loader_test))\n",
    "\n",
    "X = Variable(x).cuda().transpose(1,3).contiguous()\n",
    "Y, mu, logvar = vae.forward(X)\n",
    "loss = loss_function(Y, X, mu, logvar)\n",
    "\n",
    "y=Y.cpu().data.transpose(1,3).numpy()\n",
    "for i in range(2):\n",
    "    plt.title('%s original'%i)\n",
    "    plt.imshow(x[i].numpy())\n",
    "    plt.show()\n",
    "    \n",
    "    plt.imshow(y[i])\n",
    "    plt.title('%s reconstructed'%i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-07T02:38:42.777Z"
    }
   },
   "outputs": [],
   "source": [
    "# check the balance of the two losses, in some situations we might want to reblance, e.g. if KLD>>l2_dist and it's stalled\n",
    "\n",
    "# def loss_function(recon_x, x, mu, logvar):\n",
    "#     n, c, h, w = recon_x.size()\n",
    "#     recon_x = recon_x.view(n, -1)\n",
    "#     x = x.view(n, -1)\n",
    "#     # L2 distance\n",
    "#     l2_dist = torch.sqrt(torch.sum(torch.pow(recon_x - x, 2), 1))\n",
    "#     # see Appendix B from VAE paper:\n",
    "#     # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "#     # https://arxiv.org/abs/1312.6114\n",
    "#     # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "#     KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), 1)\n",
    "#     print(l2_dist, KLD)\n",
    "#     return torch.mean(l2_dist + KLD)\n",
    "# loss_function(Y, X, mu, logvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-06T23:12:37.247620Z",
     "start_time": "2018-05-06T23:12:37.217903Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-07T02:38:42.781Z"
    }
   },
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
