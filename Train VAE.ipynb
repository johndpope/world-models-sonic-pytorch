{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "If it stalls around loss 60, you might need a smaller batch size, e.g. 8\n",
    "\n",
    "\n",
    "\n",
    "## Log\n",
    "\n",
    "### 20180506\n",
    "Why is this not working? It stalls around loss=60 and just reconstructs the same mean image each time.\n",
    "\n",
    "- batchnorm and pad? no lots of other models use them\n",
    "    - https://github.com/josephsmann/UnsupervisedDeepLearning-Pytorch/blob/jm/udlp/autoencoder/convVAE.py#L19\n",
    "    - https://github.com/taey16/pix2pixBEGAN.pytorch/blob/master/models/BEGAN.py\n",
    "- relu, same\n",
    "- lr - similar to other models\n",
    "- inner params - similar but larger than other working models\n",
    "- batch? Maybe it seems better with a lower batch (e.g. 8 instead of 32)\n",
    "- loss, this seems right\n",
    "    - except some people use a loss balance https://github.com/AppliedDataSciencePartners/WorldModels/blob/master/vae/arch.py#L97\n",
    "        where they multuply reconstruction loss by 10 but i've got ~100 vs ~1e-4 so I don't really need to!\n",
    "        \n",
    "- ok a lower batch size seemed to get it over that initial hump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T01:04:06.166845Z",
     "start_time": "2018-05-07T01:04:06.164116Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T01:04:06.572612Z",
     "start_time": "2018-05-07T01:04:06.168238Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-06T00:21:02.585280Z",
     "start_time": "2018-05-06T00:21:02.474215Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T01:04:07.047214Z",
     "start_time": "2018-05-07T01:04:06.574174Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wassname/.pyenv/versions/3.5.3/envs/jupyter3/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "import torch.utils.data\n",
    "\n",
    "# load as dask array\n",
    "import dask.array as da\n",
    "import dask\n",
    "import h5py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T01:04:07.378578Z",
     "start_time": "2018-05-07T01:04:07.048912Z"
    }
   },
   "outputs": [],
   "source": [
    "from vae import VAE, loss_function\n",
    "from helpers.summarize import TorchSummarizeDf\n",
    "from helpers.dataset import NumpyDataset, TQDMDaskProgressBar, load_npzs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T01:04:07.512102Z",
     "start_time": "2018-05-07T01:04:07.380682Z"
    }
   },
   "outputs": [],
   "source": [
    "env_name='sonic'\n",
    "cuda= torch.cuda.is_available()\n",
    "num_epochs=200\n",
    "batch_size=6\n",
    "data_cache_file = '/MLDATA/sonic/vae.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T01:04:07.544371Z",
     "start_time": "2018-05-07T01:04:07.514705Z"
    }
   },
   "outputs": [],
   "source": [
    "# load as dask array\n",
    "\n",
    "filenames = sorted(glob.glob('./data/vae/obs_data_' + env_name + '_*.npz'))\n",
    "\n",
    "if not os.path.isfile(data_cache_file):\n",
    "    data_train = load_npzs(filenames)\n",
    "    print(data_train)\n",
    "    with TQDMDaskProgressBar():\n",
    "        da.to_hdf5(data_cache_file, '/x', data_train)\n",
    "       \n",
    "    # clear mem\n",
    "    del data_train \n",
    "    import gc\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-06T07:11:28.978952Z",
     "start_time": "2018-05-06T07:11:28.772281Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T01:04:07.576200Z",
     "start_time": "2018-05-07T01:04:07.548626Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dask.array<getitem, shape=(216000, 128, 128, 3), dtype=float32, chunksize=(2000, 128, 128, 3)>,\n",
       " dask.array<getitem, shape=(54000, 128, 128, 3), dtype=float32, chunksize=(2000, 128, 128, 3)>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load\n",
    "data = da.from_array(h5py.File(data_cache_file)['x'], chunks=(2000, 128, 128, 3))\n",
    "data\n",
    "data_split = int(len(data)*0.8)\n",
    "data_train = data[:data_split]\n",
    "data_test = data[data_split:]\n",
    "data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T01:04:07.602347Z",
     "start_time": "2018-05-07T01:04:07.578790Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<helpers.dataset.NumpyDataset at 0x7f857cf324a8>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f857cf215c0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "   \n",
    "dataset_train = NumpyDataset(data_train)\n",
    "loader_train = torch.utils.data.DataLoader(dataset_train, pin_memory=True, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "\n",
    "dataset_test = NumpyDataset(data_test)\n",
    "loader_test = torch.utils.data.DataLoader(dataset_test, pin_memory=True, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "dataset_train, loader_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-06T01:28:34.496506Z",
     "start_time": "2018-05-06T01:28:27.517Z"
    }
   },
   "source": [
    "# View model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T01:04:10.034283Z",
     "start_time": "2018-05-07T01:04:07.604473Z"
    }
   },
   "outputs": [],
   "source": [
    "vae = VAE(image_size=128, z_dim=32, conv_dim=64, code_dim=8, k_dim=128).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T01:04:25.951662Z",
     "start_time": "2018-05-07T01:04:25.905564Z"
    }
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load('./models/VAE_state_dict.pkl')\n",
    "vae.load_state_dict(vae.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T01:04:30.428690Z",
     "start_time": "2018-05-07T01:04:29.019500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 128, 128])\n",
      "Variable containing:\n",
      " 248.6288\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Total parameters 788736\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>class_name</th>\n",
       "      <th>input_shape</th>\n",
       "      <th>output_shape</th>\n",
       "      <th>nb_params</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>encoder.0.conv</td>\n",
       "      <td>Conv2d</td>\n",
       "      <td>[[(-1, 3, 128, 128)]]</td>\n",
       "      <td>[[(-1, 64, 128, 128)]]</td>\n",
       "      <td>1792</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>encoder.0.bn</td>\n",
       "      <td>BatchNorm2d</td>\n",
       "      <td>[[(-1, 64, 128, 128)]]</td>\n",
       "      <td>[[(-1, 64, 128, 128)]]</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>encoder.0.act</td>\n",
       "      <td>ReLU</td>\n",
       "      <td>[[(-1, 64, 128, 128)]]</td>\n",
       "      <td>[[(-1, 64, 128, 128)]]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>encoder.0</td>\n",
       "      <td>ConvBlock</td>\n",
       "      <td>[[(-1, 3, 128, 128)]]</td>\n",
       "      <td>[[(-1, 64, 128, 128)]]</td>\n",
       "      <td>1920</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>encoder.1.conv</td>\n",
       "      <td>Conv2d</td>\n",
       "      <td>[[(-1, 64, 128, 128)]]</td>\n",
       "      <td>[[(-1, 128, 64, 64)]]</td>\n",
       "      <td>131200</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>encoder.1.bn</td>\n",
       "      <td>BatchNorm2d</td>\n",
       "      <td>[[(-1, 128, 64, 64)]]</td>\n",
       "      <td>[[(-1, 128, 64, 64)]]</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>encoder.1.act</td>\n",
       "      <td>ReLU</td>\n",
       "      <td>[[(-1, 128, 64, 64)]]</td>\n",
       "      <td>[[(-1, 128, 64, 64)]]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>encoder.1</td>\n",
       "      <td>ConvBlock</td>\n",
       "      <td>[[(-1, 64, 128, 128)]]</td>\n",
       "      <td>[[(-1, 128, 64, 64)]]</td>\n",
       "      <td>131456</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>encoder.2.conv</td>\n",
       "      <td>Conv2d</td>\n",
       "      <td>[[(-1, 128, 64, 64)]]</td>\n",
       "      <td>[[(-1, 192, 32, 32)]]</td>\n",
       "      <td>393408</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>encoder.2.bn</td>\n",
       "      <td>BatchNorm2d</td>\n",
       "      <td>[[(-1, 192, 32, 32)]]</td>\n",
       "      <td>[[(-1, 192, 32, 32)]]</td>\n",
       "      <td>384</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>encoder.2.act</td>\n",
       "      <td>ReLU</td>\n",
       "      <td>[[(-1, 192, 32, 32)]]</td>\n",
       "      <td>[[(-1, 192, 32, 32)]]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>encoder.2</td>\n",
       "      <td>ConvBlock</td>\n",
       "      <td>[[(-1, 128, 64, 64)]]</td>\n",
       "      <td>[[(-1, 192, 32, 32)]]</td>\n",
       "      <td>393792</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>encoder.3.conv</td>\n",
       "      <td>Conv2d</td>\n",
       "      <td>[[(-1, 192, 32, 32)]]</td>\n",
       "      <td>[[(-1, 256, 16, 16)]]</td>\n",
       "      <td>786688</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>encoder.3.bn</td>\n",
       "      <td>BatchNorm2d</td>\n",
       "      <td>[[(-1, 256, 16, 16)]]</td>\n",
       "      <td>[[(-1, 256, 16, 16)]]</td>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>encoder.3.act</td>\n",
       "      <td>ReLU</td>\n",
       "      <td>[[(-1, 256, 16, 16)]]</td>\n",
       "      <td>[[(-1, 256, 16, 16)]]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>encoder.3</td>\n",
       "      <td>ConvBlock</td>\n",
       "      <td>[[(-1, 192, 32, 32)]]</td>\n",
       "      <td>[[(-1, 256, 16, 16)]]</td>\n",
       "      <td>787200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>encoder.4.conv</td>\n",
       "      <td>Conv2d</td>\n",
       "      <td>[[(-1, 256, 16, 16)]]</td>\n",
       "      <td>[[(-1, 320, 8, 8)]]</td>\n",
       "      <td>1311040</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>encoder.4.bn</td>\n",
       "      <td>BatchNorm2d</td>\n",
       "      <td>[[(-1, 320, 8, 8)]]</td>\n",
       "      <td>[[(-1, 320, 8, 8)]]</td>\n",
       "      <td>640</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>encoder.4.act</td>\n",
       "      <td>ReLU</td>\n",
       "      <td>[[(-1, 320, 8, 8)]]</td>\n",
       "      <td>[[(-1, 320, 8, 8)]]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>encoder.4</td>\n",
       "      <td>ConvBlock</td>\n",
       "      <td>[[(-1, 256, 16, 16)]]</td>\n",
       "      <td>[[(-1, 320, 8, 8)]]</td>\n",
       "      <td>1311680</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>encoder.5</td>\n",
       "      <td>Conv2d</td>\n",
       "      <td>[[(-1, 320, 8, 8)]]</td>\n",
       "      <td>[[(-1, 32, 8, 8)]]</td>\n",
       "      <td>10272</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>mu</td>\n",
       "      <td>Linear</td>\n",
       "      <td>[[(-1, 2048)]]</td>\n",
       "      <td>[[(-1, 128)]]</td>\n",
       "      <td>262272</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>logvar</td>\n",
       "      <td>Linear</td>\n",
       "      <td>[[(-1, 2048)]]</td>\n",
       "      <td>[[(-1, 128)]]</td>\n",
       "      <td>262272</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>z</td>\n",
       "      <td>Linear</td>\n",
       "      <td>[[(-1, 128)]]</td>\n",
       "      <td>[[(-1, 2048)]]</td>\n",
       "      <td>264192</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>decoder.0.deconv</td>\n",
       "      <td>ConvTranspose2d</td>\n",
       "      <td>[[(-1, 32, 8, 8)]]</td>\n",
       "      <td>[[(-1, 320, 8, 8)]]</td>\n",
       "      <td>10560</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>decoder.0.bn</td>\n",
       "      <td>BatchNorm2d</td>\n",
       "      <td>[[(-1, 320, 8, 8)]]</td>\n",
       "      <td>[[(-1, 320, 8, 8)]]</td>\n",
       "      <td>640</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>decoder.0.act</td>\n",
       "      <td>ReLU</td>\n",
       "      <td>[[(-1, 320, 8, 8)]]</td>\n",
       "      <td>[[(-1, 320, 8, 8)]]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>decoder.0</td>\n",
       "      <td>DeconvBlock</td>\n",
       "      <td>[[(-1, 32, 8, 8)]]</td>\n",
       "      <td>[[(-1, 320, 8, 8)]]</td>\n",
       "      <td>11200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>decoder.1.deconv</td>\n",
       "      <td>ConvTranspose2d</td>\n",
       "      <td>[[(-1, 320, 8, 8)]]</td>\n",
       "      <td>[[(-1, 256, 16, 16)]]</td>\n",
       "      <td>1310976</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>decoder.1.bn</td>\n",
       "      <td>BatchNorm2d</td>\n",
       "      <td>[[(-1, 256, 16, 16)]]</td>\n",
       "      <td>[[(-1, 256, 16, 16)]]</td>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>decoder.1.act</td>\n",
       "      <td>ReLU</td>\n",
       "      <td>[[(-1, 256, 16, 16)]]</td>\n",
       "      <td>[[(-1, 256, 16, 16)]]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>decoder.1</td>\n",
       "      <td>DeconvBlock</td>\n",
       "      <td>[[(-1, 320, 8, 8)]]</td>\n",
       "      <td>[[(-1, 256, 16, 16)]]</td>\n",
       "      <td>1311488</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>decoder.2.deconv</td>\n",
       "      <td>ConvTranspose2d</td>\n",
       "      <td>[[(-1, 256, 16, 16)]]</td>\n",
       "      <td>[[(-1, 192, 32, 32)]]</td>\n",
       "      <td>786624</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>decoder.2.bn</td>\n",
       "      <td>BatchNorm2d</td>\n",
       "      <td>[[(-1, 192, 32, 32)]]</td>\n",
       "      <td>[[(-1, 192, 32, 32)]]</td>\n",
       "      <td>384</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>decoder.2.act</td>\n",
       "      <td>ReLU</td>\n",
       "      <td>[[(-1, 192, 32, 32)]]</td>\n",
       "      <td>[[(-1, 192, 32, 32)]]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>decoder.2</td>\n",
       "      <td>DeconvBlock</td>\n",
       "      <td>[[(-1, 256, 16, 16)]]</td>\n",
       "      <td>[[(-1, 192, 32, 32)]]</td>\n",
       "      <td>787008</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>decoder.3.deconv</td>\n",
       "      <td>ConvTranspose2d</td>\n",
       "      <td>[[(-1, 192, 32, 32)]]</td>\n",
       "      <td>[[(-1, 128, 64, 64)]]</td>\n",
       "      <td>393344</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>decoder.3.bn</td>\n",
       "      <td>BatchNorm2d</td>\n",
       "      <td>[[(-1, 128, 64, 64)]]</td>\n",
       "      <td>[[(-1, 128, 64, 64)]]</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>decoder.3.act</td>\n",
       "      <td>ReLU</td>\n",
       "      <td>[[(-1, 128, 64, 64)]]</td>\n",
       "      <td>[[(-1, 128, 64, 64)]]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>decoder.3</td>\n",
       "      <td>DeconvBlock</td>\n",
       "      <td>[[(-1, 192, 32, 32)]]</td>\n",
       "      <td>[[(-1, 128, 64, 64)]]</td>\n",
       "      <td>393600</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>decoder.4.deconv</td>\n",
       "      <td>ConvTranspose2d</td>\n",
       "      <td>[[(-1, 128, 64, 64)]]</td>\n",
       "      <td>[[(-1, 64, 128, 128)]]</td>\n",
       "      <td>131136</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>decoder.4.bn</td>\n",
       "      <td>BatchNorm2d</td>\n",
       "      <td>[[(-1, 64, 128, 128)]]</td>\n",
       "      <td>[[(-1, 64, 128, 128)]]</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>decoder.4.act</td>\n",
       "      <td>ReLU</td>\n",
       "      <td>[[(-1, 64, 128, 128)]]</td>\n",
       "      <td>[[(-1, 64, 128, 128)]]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>decoder.4</td>\n",
       "      <td>DeconvBlock</td>\n",
       "      <td>[[(-1, 128, 64, 64)]]</td>\n",
       "      <td>[[(-1, 64, 128, 128)]]</td>\n",
       "      <td>131264</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>decoder.5</td>\n",
       "      <td>Conv2d</td>\n",
       "      <td>[[(-1, 64, 128, 128)]]</td>\n",
       "      <td>[[(-1, 3, 128, 128)]]</td>\n",
       "      <td>1731</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>Sigmoid</td>\n",
       "      <td>[[(-1, 3, 128, 128)]]</td>\n",
       "      <td>[[(-1, 3, 128, 128)]]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name       class_name             input_shape  \\\n",
       "1     encoder.0.conv           Conv2d   [[(-1, 3, 128, 128)]]   \n",
       "2       encoder.0.bn      BatchNorm2d  [[(-1, 64, 128, 128)]]   \n",
       "3      encoder.0.act             ReLU  [[(-1, 64, 128, 128)]]   \n",
       "4          encoder.0        ConvBlock   [[(-1, 3, 128, 128)]]   \n",
       "5     encoder.1.conv           Conv2d  [[(-1, 64, 128, 128)]]   \n",
       "6       encoder.1.bn      BatchNorm2d   [[(-1, 128, 64, 64)]]   \n",
       "7      encoder.1.act             ReLU   [[(-1, 128, 64, 64)]]   \n",
       "8          encoder.1        ConvBlock  [[(-1, 64, 128, 128)]]   \n",
       "9     encoder.2.conv           Conv2d   [[(-1, 128, 64, 64)]]   \n",
       "10      encoder.2.bn      BatchNorm2d   [[(-1, 192, 32, 32)]]   \n",
       "11     encoder.2.act             ReLU   [[(-1, 192, 32, 32)]]   \n",
       "12         encoder.2        ConvBlock   [[(-1, 128, 64, 64)]]   \n",
       "13    encoder.3.conv           Conv2d   [[(-1, 192, 32, 32)]]   \n",
       "14      encoder.3.bn      BatchNorm2d   [[(-1, 256, 16, 16)]]   \n",
       "15     encoder.3.act             ReLU   [[(-1, 256, 16, 16)]]   \n",
       "16         encoder.3        ConvBlock   [[(-1, 192, 32, 32)]]   \n",
       "17    encoder.4.conv           Conv2d   [[(-1, 256, 16, 16)]]   \n",
       "18      encoder.4.bn      BatchNorm2d     [[(-1, 320, 8, 8)]]   \n",
       "19     encoder.4.act             ReLU     [[(-1, 320, 8, 8)]]   \n",
       "20         encoder.4        ConvBlock   [[(-1, 256, 16, 16)]]   \n",
       "21         encoder.5           Conv2d     [[(-1, 320, 8, 8)]]   \n",
       "22                mu           Linear          [[(-1, 2048)]]   \n",
       "23            logvar           Linear          [[(-1, 2048)]]   \n",
       "24                 z           Linear           [[(-1, 128)]]   \n",
       "25  decoder.0.deconv  ConvTranspose2d      [[(-1, 32, 8, 8)]]   \n",
       "26      decoder.0.bn      BatchNorm2d     [[(-1, 320, 8, 8)]]   \n",
       "27     decoder.0.act             ReLU     [[(-1, 320, 8, 8)]]   \n",
       "28         decoder.0      DeconvBlock      [[(-1, 32, 8, 8)]]   \n",
       "29  decoder.1.deconv  ConvTranspose2d     [[(-1, 320, 8, 8)]]   \n",
       "30      decoder.1.bn      BatchNorm2d   [[(-1, 256, 16, 16)]]   \n",
       "31     decoder.1.act             ReLU   [[(-1, 256, 16, 16)]]   \n",
       "32         decoder.1      DeconvBlock     [[(-1, 320, 8, 8)]]   \n",
       "33  decoder.2.deconv  ConvTranspose2d   [[(-1, 256, 16, 16)]]   \n",
       "34      decoder.2.bn      BatchNorm2d   [[(-1, 192, 32, 32)]]   \n",
       "35     decoder.2.act             ReLU   [[(-1, 192, 32, 32)]]   \n",
       "36         decoder.2      DeconvBlock   [[(-1, 256, 16, 16)]]   \n",
       "37  decoder.3.deconv  ConvTranspose2d   [[(-1, 192, 32, 32)]]   \n",
       "38      decoder.3.bn      BatchNorm2d   [[(-1, 128, 64, 64)]]   \n",
       "39     decoder.3.act             ReLU   [[(-1, 128, 64, 64)]]   \n",
       "40         decoder.3      DeconvBlock   [[(-1, 192, 32, 32)]]   \n",
       "41  decoder.4.deconv  ConvTranspose2d   [[(-1, 128, 64, 64)]]   \n",
       "42      decoder.4.bn      BatchNorm2d  [[(-1, 64, 128, 128)]]   \n",
       "43     decoder.4.act             ReLU  [[(-1, 64, 128, 128)]]   \n",
       "44         decoder.4      DeconvBlock   [[(-1, 128, 64, 64)]]   \n",
       "45         decoder.5           Conv2d  [[(-1, 64, 128, 128)]]   \n",
       "46           sigmoid          Sigmoid   [[(-1, 3, 128, 128)]]   \n",
       "\n",
       "              output_shape  nb_params  level  \n",
       "1   [[(-1, 64, 128, 128)]]       1792      2  \n",
       "2   [[(-1, 64, 128, 128)]]        128      2  \n",
       "3   [[(-1, 64, 128, 128)]]          0      2  \n",
       "4   [[(-1, 64, 128, 128)]]       1920      1  \n",
       "5    [[(-1, 128, 64, 64)]]     131200      2  \n",
       "6    [[(-1, 128, 64, 64)]]        256      2  \n",
       "7    [[(-1, 128, 64, 64)]]          0      2  \n",
       "8    [[(-1, 128, 64, 64)]]     131456      1  \n",
       "9    [[(-1, 192, 32, 32)]]     393408      2  \n",
       "10   [[(-1, 192, 32, 32)]]        384      2  \n",
       "11   [[(-1, 192, 32, 32)]]          0      2  \n",
       "12   [[(-1, 192, 32, 32)]]     393792      1  \n",
       "13   [[(-1, 256, 16, 16)]]     786688      2  \n",
       "14   [[(-1, 256, 16, 16)]]        512      2  \n",
       "15   [[(-1, 256, 16, 16)]]          0      2  \n",
       "16   [[(-1, 256, 16, 16)]]     787200      1  \n",
       "17     [[(-1, 320, 8, 8)]]    1311040      2  \n",
       "18     [[(-1, 320, 8, 8)]]        640      2  \n",
       "19     [[(-1, 320, 8, 8)]]          0      2  \n",
       "20     [[(-1, 320, 8, 8)]]    1311680      1  \n",
       "21      [[(-1, 32, 8, 8)]]      10272      1  \n",
       "22           [[(-1, 128)]]     262272      0  \n",
       "23           [[(-1, 128)]]     262272      0  \n",
       "24          [[(-1, 2048)]]     264192      0  \n",
       "25     [[(-1, 320, 8, 8)]]      10560      2  \n",
       "26     [[(-1, 320, 8, 8)]]        640      2  \n",
       "27     [[(-1, 320, 8, 8)]]          0      2  \n",
       "28     [[(-1, 320, 8, 8)]]      11200      1  \n",
       "29   [[(-1, 256, 16, 16)]]    1310976      2  \n",
       "30   [[(-1, 256, 16, 16)]]        512      2  \n",
       "31   [[(-1, 256, 16, 16)]]          0      2  \n",
       "32   [[(-1, 256, 16, 16)]]    1311488      1  \n",
       "33   [[(-1, 192, 32, 32)]]     786624      2  \n",
       "34   [[(-1, 192, 32, 32)]]        384      2  \n",
       "35   [[(-1, 192, 32, 32)]]          0      2  \n",
       "36   [[(-1, 192, 32, 32)]]     787008      1  \n",
       "37   [[(-1, 128, 64, 64)]]     393344      2  \n",
       "38   [[(-1, 128, 64, 64)]]        256      2  \n",
       "39   [[(-1, 128, 64, 64)]]          0      2  \n",
       "40   [[(-1, 128, 64, 64)]]     393600      1  \n",
       "41  [[(-1, 64, 128, 128)]]     131136      2  \n",
       "42  [[(-1, 64, 128, 128)]]        128      2  \n",
       "43  [[(-1, 64, 128, 128)]]          0      2  \n",
       "44  [[(-1, 64, 128, 128)]]     131264      1  \n",
       "45   [[(-1, 3, 128, 128)]]       1731      1  \n",
       "46   [[(-1, 3, 128, 128)]]          0      0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = np.random.randn(64, 64, 3)\n",
    "img = np.random.randn(64*2, 64*2, 3)\n",
    "gpu_img = Variable(torch.from_numpy(img[np.newaxis].transpose(0, 3, 1, 2))).float().cuda()\n",
    "\n",
    "with TorchSummarizeDf(vae) as tdf:\n",
    "    x, mu, logvar = vae.forward(gpu_img)\n",
    "    print(x.size())\n",
    "    print(loss_function(x, gpu_img, mu, logvar))\n",
    "    x = x.data.cpu().numpy()[0].transpose(1, 2, 0)\n",
    "    df = tdf.make_df()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T01:04:30.496349Z",
     "start_time": "2018-05-07T01:04:30.431108Z"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def train(loader, net, optimizer, loss_function, test=False, cuda=True):\n",
    "    if test:\n",
    "        net.eval()\n",
    "    else:\n",
    "        net.train()\n",
    "    info = collections.defaultdict(list)\n",
    "    \n",
    "    with tqdm(total=len(loader)*loader.batch_size, mininterval=0.5, desc='test' if test else 'training') as prog:\n",
    "        for i, (batch,) in enumerate(loader):\n",
    "            x = Variable(batch.transpose(1,3)).cuda() #*255 # FIXME (I divided by 255 once too many during gathering)\n",
    "            y, mu, logvar = vae.forward(x)\n",
    "            loss = loss_function(y, x, mu, logvar)\n",
    "            info['loss'].append(loss.cpu().data.numpy()[0])\n",
    "            \n",
    "            if not test:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            prog.update(loader.batch_size)\n",
    "            prog.desc='loss={:2.4f}'.format(np.mean(info['loss'][-100:]))\n",
    "            \n",
    "            if i%(100000//batch_size)==0:\n",
    "                print('[{}/{}] loss={:2.4f}'.format(i, len(loader), np.mean(info['loss'][-100:])))\n",
    "        print('[{}/{}] loss={:2.4f}'.format(i, len(loader), np.mean(info['loss'][-100:])))\n",
    "        prog.close()\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T01:04:30.644667Z",
     "start_time": "2018-05-07T01:04:30.599686Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot reconstructions\n",
    "def plot_results(loader=loader_test, n=2, epoch=0):\n",
    "    x, = next(iter(loader))\n",
    "\n",
    "    X = Variable(x).cuda().transpose(1,3).contiguous()\n",
    "    Y, mu, logvar = vae.forward(X)\n",
    "    loss = loss_function(Y, X, mu, logvar)\n",
    "\n",
    "    y=Y.cpu().data.transpose(1,3).numpy()\n",
    "    for i in range(n):\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title('original')\n",
    "        plt.imshow(x[i].numpy())\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(y[i])\n",
    "        plt.title('reconstructed, loss {:2.4f}'.format(loss.cpu().data.numpy()[0]))\n",
    "\n",
    "        plt.suptitle('epoch {}, index {}, original'.format(epoch, i))\n",
    "        plt.show()\n",
    "        \n",
    "# plot_results(loader=loader_test, n=2, epoch=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T01:04:31.015840Z",
     "start_time": "2018-05-07T01:04:30.991451Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "import torch.optim.lr_scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=6, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T01:04:31.663456Z",
     "start_time": "2018-05-07T01:04:31.641474Z"
    }
   },
   "outputs": [],
   "source": [
    "infos=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T01:10:10.691237Z",
     "start_time": "2018-05-07T01:04:32.007549Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3456f705c74146e99e0d9438968def09"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/36000] loss=81.0109\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-698eaadff7a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0minfo_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-50bb7daa51ae>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(loader, net, optimizer, loss_function, test, cuda)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#*255 # FIXME (I divided by 255 once too many during gathering)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/oldhome/wassname/Documents/projects/retro_sonic_comp/world-models-pytorch/vae.py\u001b[0m in \u001b[0;36mloss_function\u001b[0;34m(recon_x, x, mu, logvar)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;31m# 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mKLD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml2_dist\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mKLD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.3/envs/jupyter3/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(self, dim, keepdim)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mMean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.3/envs/jupyter3/lib/python3.5/site-packages/torch/autograd/_functions/reduce.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, input, dim, keepdim)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkeepdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs): \n",
    "    info = train(loader_train, vae, optimizer, loss_function, test=False, cuda=True)\n",
    "    info_val = train(loader_test, vae, optimizer, loss_function, test=True, cuda=True)\n",
    "    scheduler.step(np.mean(info_val['loss']))\n",
    "    \n",
    "    print('Epoch {}, loss={:2.4f}, loss_val={:2.4f}'.format(epoch, np.mean(info['loss']), np.mean(info_val['loss'])))\n",
    "    infos.append([info, info_val])\n",
    "    \n",
    "    plot_results(loader=loader_test, n=2, epoch=epoch)\n",
    "    \n",
    "    torch.save(vae, './models/VAE_{}.pkl'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T01:04:10.212330Z",
     "start_time": "2018-05-07T01:04:05.981Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(vae, './models/VAE.pkl')\n",
    "torch.save(vae.state_dict(), './models/VAE_state_dict.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T01:04:10.213259Z",
     "start_time": "2018-05-07T01:04:05.985Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot history\n",
    "histories = []\n",
    "for info, info_val in infos:\n",
    "    history = {k+'_val':np.mean(v) for k,v in info_val.items()}\n",
    "    history.update({k:np.mean(v) for k,v in info.items()})\n",
    "    histories.append(history)\n",
    "histories = pd.DataFrame(histories)\n",
    "histories.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T01:04:10.213963Z",
     "start_time": "2018-05-07T01:04:05.992Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot reconstructions\n",
    "x, = next(iter(loader_test))\n",
    "\n",
    "X = Variable(x).cuda().transpose(1,3).contiguous()\n",
    "Y, mu, logvar = vae.forward(X)\n",
    "loss = loss_function(Y, X, mu, logvar)\n",
    "\n",
    "y=Y.cpu().data.transpose(1,3).numpy()\n",
    "for i in range(2):\n",
    "    plt.title('%s original'%i)\n",
    "    plt.imshow(x[i].numpy())\n",
    "    plt.show()\n",
    "    \n",
    "    plt.imshow(y[i])\n",
    "    plt.title('%s reconstructed'%i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T01:04:10.214720Z",
     "start_time": "2018-05-07T01:04:05.996Z"
    }
   },
   "outputs": [],
   "source": [
    "# check the balance of the two losses, in some situations we might want to reblance, e.g. if KLD>>l2_dist and it's stalled\n",
    "\n",
    "# def loss_function(recon_x, x, mu, logvar):\n",
    "#     n, c, h, w = recon_x.size()\n",
    "#     recon_x = recon_x.view(n, -1)\n",
    "#     x = x.view(n, -1)\n",
    "#     # L2 distance\n",
    "#     l2_dist = torch.sqrt(torch.sum(torch.pow(recon_x - x, 2), 1))\n",
    "#     # see Appendix B from VAE paper:\n",
    "#     # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "#     # https://arxiv.org/abs/1312.6114\n",
    "#     # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "#     KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), 1)\n",
    "#     print(l2_dist, KLD)\n",
    "#     return torch.mean(l2_dist + KLD)\n",
    "# loss_function(Y, X, mu, logvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-06T23:12:37.247620Z",
     "start_time": "2018-05-06T23:12:37.217903Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T01:04:10.215631Z",
     "start_time": "2018-05-07T01:04:06.004Z"
    }
   },
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter3",
   "language": "python",
   "name": "jupyter3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
